{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA_Experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKrj3MT4lD2Kr+S6qstf+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfisberg/zhaw-ba-online/blob/main/scripts/experiment_player/experiment_player.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_abPOY0HB_x"
      },
      "source": [
        "!pip install mir_eval\n",
        "!pip install rt_pie_lib\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjY4kcbEJ1GR"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ7F5MEeKloY"
      },
      "source": [
        "## CREPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w_Bl9RJLreO"
      },
      "source": [
        "### CREPE 2048 / 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VqwGrubLvkR"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LefCD0tgJ6XJ"
      },
      "source": [
        "###CREPE 1024 / 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdQj8-QSJ4Fl"
      },
      "source": [
        "true_hz = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/crepe_1024/crepe_true_hz_1024_512.npy')\n",
        "predicted_hz = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/crepe_1024/crepe_predicted_hz_1024_512.npy')\n",
        "true_vector = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/crepe_1024/crepe_true_vector_1024_512.npy')\n",
        "pred_vector = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/crepe_1024/crepe_pred_vector_1024_512.npy')\n",
        "diff = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/crepe_1024/crepe_diff_1024_512.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKAoQuceKeTg"
      },
      "source": [
        "### CREPE 512 / 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhMDthBFKdv1"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVLPwj2JKp8X"
      },
      "source": [
        "### CREPE 256 / 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4JJYLUzKpG4"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9zC9gZcKv3f"
      },
      "source": [
        "## DeepF0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-UDPIwKzsq"
      },
      "source": [
        "### DeepF0 2048 / 1024"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYDQ742EKuIp"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HycfH7FLLEaF"
      },
      "source": [
        "### DeepF0 1024 / 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuA37qa_K7y7"
      },
      "source": [
        "true_hz = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/deepf0_1024/deepf0_true_hz_1024_512.npy')\n",
        "predicted_hz = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/deepf0_1024/deepf0_predicted_hz_1024_512.npy')\n",
        "true_vector = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/deepf0_1024/deepf0_true_vector_1024_512.npy')\n",
        "pred_vector = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/deepf0_1024/deepf0_pred_vector_1024_512.npy')\n",
        "diff = np.load('/content/drive/MyDrive/BA_2021/saved_vectors/deepf0_1024/deepf0_diff_1024_512.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6JVRxBxLINU"
      },
      "source": [
        "### DeepF0 512 / 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhxoWGV7K8QF"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhsEl86ILKw_"
      },
      "source": [
        "### DeepF0 256 / 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIq6JPsuK8ak"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDqVb68iK9p3"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sXL48XHLOws"
      },
      "source": [
        "### LSTM 1024 / 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3otXU90eLCom"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeBG5V-kLQ9S"
      },
      "source": [
        "### LSTM 512 / 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lJcts_YLCwy"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_pLLTNoLWBf"
      },
      "source": [
        "### LSTM 256 / 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SrK26jnLC6a"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF-dxa_CLZWX"
      },
      "source": [
        "### LSTM reduced Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HmblKKULDM3"
      },
      "source": [
        "true_hz = np.load()\n",
        "predicted_hz = np.load()\n",
        "true_vector = np.load()\n",
        "pred_vector = np.load()\n",
        "diff = np.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX53n1x3LiOi"
      },
      "source": [
        "# Load Local Average Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxPH7q__LjON"
      },
      "source": [
        "from scipy.signal import argrelextrema\n",
        "def convert_bin_to_local_average_cents(salience, center=None):\n",
        "    \"\"\"\n",
        "    find the weighted average cents near the argmax bin\n",
        "    \"\"\"\n",
        "    if not hasattr(convert_bin_to_local_average_cents, 'cents_mapping'):\n",
        "        # the bin number-to-cents mapping\n",
        "        convert_bin_to_local_average_cents.cents_mapping = (\n",
        "                np.linspace(0, 7180, 360) + 2051.14876287)\n",
        "    if salience.ndim == 1:\n",
        "        center = int(np.argmax(salience))\n",
        "        start = max(0, center - 4)\n",
        "        end = min(len(salience), center + 5)\n",
        "        salience = salience[start:end]\n",
        "        product_sum = np.sum(\n",
        "            salience * convert_bin_to_local_average_cents.cents_mapping[start:end])\n",
        "        weight_sum = np.sum(salience)\n",
        "        return product_sum / weight_sum\n",
        "    if salience.ndim == 2:\n",
        "        return np.array([convert_bin_to_local_average_cents(salience[i, :]) for i in\n",
        "                         range(salience.shape[0])])\n",
        "    raise Exception(\"Label should be either 1d or 2d ndarray.\")\n",
        "\n",
        "\n",
        "def convert_bin_to_local_average_cents_lowest_maxima(salience, center=None, maxima_order=5, maxima_minval=0.2, tolerance=0.1):\n",
        "    \"\"\"\n",
        "    find the weighted average cents near the argmax bin todo\n",
        "    \"\"\"\n",
        "    if salience.ndim == 1:\n",
        "        \n",
        "        if salience[0] > 0.2:\n",
        "            salience = __create_maximum_bin(0)\n",
        "            return convert_bin_to_local_average_cents(np.squeeze(salience), center=center)\n",
        "        \n",
        "        else:\n",
        "            maxima = argrelextrema(salience, np.greater, order=maxima_order)[0]\n",
        "        # maxima = np.argmax(salience)\n",
        "        # if maxima == 0 and salience[0] < 0.8:    \n",
        "        #     maxima = argrelextrema(salience, np.greater, order=maxima_order)[0]\n",
        "        #     maxima = [x if x >= 51 and x <= 217 else 0 for x in maxima]\n",
        "        #     maxima = np.max(maxima)\n",
        "        #     salience = __create_maximum_bin(maxima)\n",
        "        maxima = [(x, converters.convert_cent_to_hz(convert_bin_to_local_average_cents(__create_maximum_bin(x))))\n",
        "                  for x in maxima if salience[x] >= maxima_minval]\n",
        "        if len(maxima) > 1:\n",
        "            success, idx = __try_find_f0_in_maxima(maxima, tolerance=tolerance)\n",
        "            if success:\n",
        "                salience = np.zeros(360)\n",
        "                salience[maxima[idx][0]] = 1\n",
        "        return convert_bin_to_local_average_cents(salience, center=center)\n",
        "\n",
        "    if salience.ndim == 2:\n",
        "        return np.array([convert_bin_to_local_average_cents_lowest_maxima(salience[i, :]) for i in\n",
        "                         range(salience.shape[0])])\n",
        "\n",
        "    raise Exception(\"Label should be either 1d or 2d ndarray.\")\n",
        "\n",
        "\n",
        "def __create_maximum_bin(index):\n",
        "    b = np.zeros(360)\n",
        "    b[index] = 1\n",
        "    return b\n",
        "\n",
        "def __try_find_f0_in_maxima_true_negativs(maxima):\n",
        "    maxima.sort(key=lambda x: x[1])\n",
        "    for i in range (len(maxima) -1):\n",
        "        max_current = maxima[i][1]\n",
        "\n",
        "\n",
        "def __try_find_f0_in_maxima(maxima, tolerance=0.1):\n",
        "    maxima.sort(key=lambda x: x[1])\n",
        "    for i in range(len(maxima) - 1):\n",
        "        max_current = maxima[i][1]\n",
        "        max_next = maxima[i + 1][1]\n",
        "        rel_diff = abs(max_current * 2 - max_next) / max_next\n",
        "        if rel_diff <= tolerance:\n",
        "            return True, i\n",
        "    return False, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgw7On2JZ8dC"
      },
      "source": [
        "# Load all other methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjPfZAfcNfm8"
      },
      "source": [
        "from rt_pie_lib import converters\n",
        "def vector_to_hz(pred_vector):\n",
        "    pred_cents = []\n",
        "    for i in range(len(pred_vector)):\n",
        "        pred_c = convert_bin_to_local_average_cents_lowest_maxima(pred_vector[i])\n",
        "        pred_cents.append(pred_c)\n",
        "    \n",
        "    pred_cents = np.array(pred_cents)\n",
        "    pred_hz = converters.convert_cent_to_hz(pred_cents)\n",
        "    return pred_hz\n",
        "\n",
        "def calc_new_diff(true_hz, pred_hz):\n",
        "    diff = true_hz - pred_hz\n",
        "    return diff\n",
        "\n",
        "def histogram(diff):\n",
        "    plt.rcParams.update({'font.size': 22})  \n",
        "    n_bins = 250\n",
        "    x = diff\n",
        "\n",
        "    plt.figure(figsize=[16,9])\n",
        "    plt.hist(x, bins=n_bins)\n",
        "    #plt.xlim([-200, 200])\n",
        "    plt.ylim([0, 30000])\n",
        "    plt.axvline(np.median(x), color='k', linestyle='dashed', linewidth=2, label='MED')\n",
        "    plt.axvline(np.mean(x), color='k', linestyle='solid', linewidth=2, label='MEAN')\n",
        "    plt.axvline(np.quantile(x, 0.05), color='k', linestyle='dotted', linewidth=2, label='5% quantile')\n",
        "    plt.axvline(np.quantile(x, 0.95), color='k', linestyle='dashdot', linewidth=2, label='95% quantile')\n",
        "    plt.xlabel(\"Error in Hertz\")\n",
        "    plt.ylabel(\"Number of Errors\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.rcParams.update({'font.size': 12}) \n",
        "\n",
        "def zero_pitch_analysis_cnn_models(true_hz, predicted_hz):\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "    for i in range(len(true_hz)):\n",
        "        if true_hz[i] >= 30 and true_hz[i] <= 35 and predicted_hz[i] >= 30 and predicted_hz[i] <= 35:\n",
        "            tp += 1\n",
        "            continue\n",
        "        if true_hz[i] > 35 and predicted_hz[i] > 35:\n",
        "            tn += 1\n",
        "            continue\n",
        "        if true_hz[i] >= 30 and true_hz[i] <= 35 and predicted_hz[i] > 35:\n",
        "            fn += 1\n",
        "            continue\n",
        "        if true_hz[i] > 35 and predicted_hz[i] >= 30 and predicted_hz[i] <= 35:\n",
        "            fp += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "    try:\n",
        "        sum = tp + fp + tn + fn\n",
        "        percentage_zero_truth = (tp + fn) / sum * 100\n",
        "        percentage_zero_predicted = (tp + fp) / sum * 100\n",
        "        precision = tp / (tp + fp) * 100  # Anteil unserer 0 schätzungen die richtig sind\n",
        "        recall = tp / (tp + fn) * 100  # Wieviele der tatsächlichen 0 schätzungen haben wir erwischt\n",
        "        accuracy = (tp + tn) / sum * 100  # Anteil richtige predictions\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        tn_percentage = tn / sum * 100\n",
        "        tp_percentage = tp / sum * 100\n",
        "        fp_percentage = fp / sum * 100\n",
        "        fn_percentage = fn / sum * 100\n",
        "\n",
        "        print(\"ZERO PITCH ANALYSIS\")\n",
        "        print(\"Sample size (test data set): \", sum)\n",
        "        print(\"0 - % in ground truth: \", \"%.2f\" % percentage_zero_truth)\n",
        "        print(\"0 - % in predictions: \",  \"%.2f\" % percentage_zero_predicted)\n",
        "        print(\"Accuarcy: \", \"%.2f\" % accuracy)\n",
        "        print(\"Precision: \", \"%.2f\" % precision)\n",
        "        print(\"Recall: \", \"%.2f\" % recall)\n",
        "        print(\"F1-Score\", \"%.2f\" % f1)\n",
        "        print(\"True Negatives: \", \"%.2f\" % tn_percentage)\n",
        "        print(\"True Positives: \", \"%.2f\" % tp_percentage)\n",
        "        print(\"False Positives: \", \"%.2f\" % fp_percentage)\n",
        "        print(\"False Negatives: \", \"%.2f\" % fn_percentage)\n",
        "\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Zero Pitch Analysis not possible: divide by zero\")\n",
        "\n",
        "\n",
        "def zero_pitch_analysis_lstm():\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(len(true_hz)):\n",
        "        if true_hz[i] == 0 and predicted_hz[i] == 0:\n",
        "            tp += 1\n",
        "            continue\n",
        "        if true_hz[i] > 0 and predicted_hz[i] > 0:\n",
        "            tn += 1\n",
        "            continue\n",
        "        if true_hz[i] == 0 and predicted_hz[i] > 0:\n",
        "            fn += 1\n",
        "            continue\n",
        "        if true_hz[i] > 0 and predicted_hz[i] == 0:\n",
        "            fp += 1\n",
        "            continue\n",
        "\n",
        "    try:\n",
        "        sum = tp + fp + tn + fn\n",
        "        percentage_zero_truth = (tp + fn) / sum * 100\n",
        "        percentage_zero_predicted = (tp + fp) / sum * 100\n",
        "        precision = tp / (tp + fp) * 100  # Anteil unserer 0 schätzungen die richtig sind\n",
        "        recall = tp / (tp + fn) * 100  # Wieviele der tatsächlichen 0 schätzungen haben wir erwischt\n",
        "        accuracy = (tp + tn) / sum * 100  # Anteil richtige predictions\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        tn_percentage = tn / sum * 100\n",
        "        tp_percentage = tp / sum * 100\n",
        "        fp_percentage = fp / sum * 100\n",
        "        fn_percentage = fn / sum * 100\n",
        "\n",
        "        print(\"ZERO PITCH ANALYSIS\")\n",
        "        print(\"Sample size (test data set): \", sum)\n",
        "        print(\"0 - % in ground truth: \", \"%.2f\" % percentage_zero_truth)\n",
        "        print(\"0 - % in predictions: \",  \"%.2f\" % percentage_zero_predicted)\n",
        "        print(\"Accuarcy: \", \"%.2f\" % accuracy)\n",
        "        print(\"Precision: \", \"%.2f\" % precision)\n",
        "        print(\"Recall: \", \"%.2f\" % recall)\n",
        "        print(\"F1-Score\", \"%.2f\" % f1)\n",
        "        print(\"True Negatives: \", \"%.2f\" % tn_percentage)\n",
        "        print(\"True Positives: \", \"%.2f\" % tp_percentage)\n",
        "        print(\"False Positives: \", \"%.2f\" % fp_percentage)\n",
        "        print(\"False Negatives: \", \"%.2f\" % fn_percentage)\n",
        "\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Zero Pitch Analysis not possible: divide by zero\")\n",
        "\n",
        "def filter_values(true_hz, pred_hz, filter_lower_bound, filter_upper_bound, filter_method):\n",
        "    combined = zip(true_hz, predicted_hz)\n",
        "    if filter_method == 'gt':\n",
        "        filtered = [x for x in list(combined) if x[0] > filter_lower_bound and x[0] < filter_upper_bound]\n",
        "    elif filter_method == 'pred':\n",
        "        filtered = [x for x in list(combined) if x[1] > filter_lower_bound and x[1] < filter_upper_bound]\n",
        "    elif filter_method == 'both' or filter_method == 'none':\n",
        "        filtered = [x for x in list(combined) if x[0] > filter_lower_bound and x[0] < filter_upper_bound and x[1] > filter_lower_bound and x[1] < filter_upper_bound]\n",
        "    filtered_unzipped = np.array(list(zip(*filtered)))\n",
        "    true_hz_filtered = filtered_unzipped[0]\n",
        "    pred_hz_filtered = filtered_unzipped[1]\n",
        "    diff_filtered = true_hz_filtered - pred_hz_filtered\n",
        "    return true_hz_filtered, pred_hz_filtered, diff_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0dyTKS9S1nz"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7w32VlyPy6R"
      },
      "source": [
        "from rt_pie_lib import metrics\n",
        "def main(true_hz, pred_vector, model, filter=0, filter_lower_bound=60, filter_upper_bound=400, filter_method='none'):\n",
        "    pred_hz_new = vector_to_hz(pred_vector)\n",
        "    diff_new = calc_new_diff(true_hz, pred_hz_new)\n",
        "    ##include filter if necessary\n",
        "    if filter == 1:\n",
        "        true_hz_filtered, pred_hz_filtered, diff_filtered = filter_values(true_hz, pred_hz_new, filter_lower_bound, filter_upper_bound, filter_method)\n",
        "        histogram(diff_filtered)\n",
        "        print('METRICS')\n",
        "        metrics.get_hz_metrics(true_hz_filtered, pred_hz_filtered, print_output=True)\n",
        "        print('\\n')\n",
        "        if model == 'cnn':\n",
        "            zero_pitch_analysis_cnn_models(true_hz_filtered, pred_hz_filtered)\n",
        "        elif model == 'lstm':\n",
        "            zero_pitch_analysis_lstm(true_hz_filtered, pred_hz_filtered)\n",
        "    else:\n",
        "        histogram(diff_new)\n",
        "        print('METRICS')\n",
        "        metrics.get_hz_metrics(true_hz, pred_hz_new, print_output=True)\n",
        "        print('\\n')\n",
        "        if model == 'cnn':\n",
        "            zero_pitch_analysis_cnn_models(true_hz, pred_hz_new)\n",
        "        elif model == 'lstm':\n",
        "            zero_pitch_analysis_lstm(true_hz, pred_hz_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeMWqRHQYsK7"
      },
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSpFSB-4W2Zu"
      },
      "source": [
        "main(true_hz=true_hz, pred_vector=pred_vector, model='cnn', filter=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}