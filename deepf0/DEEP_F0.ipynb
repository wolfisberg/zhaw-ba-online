{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP-F0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfisberg/zhaw-ba-online/blob/main/deepf0/DEEP_F0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU5ZvlGglGDd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q42BY8BPSoL",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05387ef-d2e0-4906-ada2-19789a2ad649"
      },
      "source": [
        "!pip install mir_eval\n",
        "!pip install rt_pie_lib\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.interpolate\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import datetime\n",
        "import mir_eval\n",
        "import math\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8nh41ellB7K"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWZIc9-WSItM",
        "collapsed": true
      },
      "source": [
        "# Audio\n",
        "SNR_RANGE = (-5.0,20.0) #dB\n",
        "FRAME_LENGTH = 2048\n",
        "FRAME_STEP = 1024\n",
        "MIN_RAND_GAIN = 0.05\n",
        "MAX_RAND_GAIN = 1.1\n",
        "SAMPLE_LENGTH = 3 #shorter than shortest noise/speech sample\n",
        "FS = 16000\n",
        "PITCH_SAMPLING_TIME = 0.01 # s\n",
        "PITCH_FRAME_LENGTH = 0.032 # s\n",
        "\n",
        "\n",
        "# Data\n",
        "BATCH_SIZE = 32\n",
        "NUM_FRAMES = 1 + (FS * SAMPLE_LENGTH - FRAME_LENGTH) // FRAME_STEP\n",
        "# NUM_FRAMES = 1\n",
        "\n",
        "# Training\n",
        "STEPS_PER_EPOCH = 500\n",
        "EPOCHS = 100\n",
        "VALIDATION_STEPS = 5\n",
        "\n",
        "\n",
        "# Directories\n",
        "_DATA_DIR = os.path.join('/content/drive/MyDrive/BA_2021/')\n",
        "_TFRECORDS_DIR = os.path.join(_DATA_DIR, 'tfrecords')\n",
        "\n",
        "SPEECH_DATA_TR_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'tr')\n",
        "NOISE_DATA_TR_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'tr')\n",
        "SPEECH_DATA_CV_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'cv')\n",
        "NOISE_DATA_CV_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'cv')\n",
        "SPEECH_DATA_TT_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'tt')\n",
        "NOISE_DATA_TT_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'tt')\n",
        "\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "# Misc\n",
        "SEED = 2\n",
        "\n",
        "\n",
        "# Parsing\n",
        "PARSING_CONFIG_NOISE = {\n",
        "    'data': tf.io.VarLenFeature(tf.string),\n",
        "    'data_sampling_rate': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_num_channels': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_width': tf.io.VarLenFeature(tf.int64),\n",
        "}\n",
        "\n",
        "PARSING_CONFIG_SPEECH = {\n",
        "    'data': tf.io.VarLenFeature(tf.string),\n",
        "    'data_sampling_rate': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_num_channels': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_width': tf.io.VarLenFeature(tf.int64),\n",
        "    'pitch': tf.io.VarLenFeature(tf.float32),\n",
        "    'pitch_confidence': tf.io.VarLenFeature(tf.float32),\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-kwHYrpmCgl"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmb9w4ACwoUi"
      },
      "source": [
        "## Copy Data to Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYKlRsmDwsKM"
      },
      "source": [
        "DATA_DIR_LOCAL = '/content/data'\n",
        "\n",
        "if not os.path.exists(DATA_DIR_LOCAL):\n",
        "    os.mkdir(DATA_DIR_LOCAL)\n",
        "    \n",
        "    RECORD_DIR_LOCAL = os.path.join(DATA_DIR_LOCAL, 'tfrecords')\n",
        "    shutil.copytree(_TFRECORDS_DIR, RECORD_DIR_LOCAL)\n",
        "\n",
        "\n",
        "_TFRECORDS_DIR = os.path.join(DATA_DIR_LOCAL, 'tfrecords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlkFt3Nvsqn-"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhwJchGumFSo"
      },
      "source": [
        "def _parse_noise_record(serialized_example):\n",
        "    parsed_features = tf.io.parse_single_example(serialized_example, features=PARSING_CONFIG_NOISE)\n",
        "    decoded_features = {\n",
        "        \"data_num_channels\": tf.cast(parsed_features[\"data_num_channels\"].values[0], tf.int32),\n",
        "        \"data_sampling_rate\": tf.cast(parsed_features[\"data_sampling_rate\"].values[0], tf.int32),\n",
        "        \"data_width\": tf.cast(parsed_features[\"data_width\"].values[0], tf.int32),\n",
        "    }\n",
        "    data = tf.io.decode_raw(parsed_features['data'].values[0], tf.int16)\n",
        "    decoded_features.update({\"data\": data})\n",
        "    return decoded_features\n",
        "\n",
        "\n",
        "def _parse_speech_record(serialized_example):\n",
        "    parsed_features = tf.io.parse_single_example(serialized_example, features=PARSING_CONFIG_SPEECH)\n",
        "    decoded_features = {\n",
        "        \"data_num_channels\": tf.cast(parsed_features[\"data_num_channels\"].values[0], tf.int32),\n",
        "        \"data_sampling_rate\": tf.cast(parsed_features[\"data_sampling_rate\"].values[0], tf.int32),\n",
        "        \"data_width\": tf.cast(parsed_features[\"data_width\"].values[0], tf.int32),\n",
        "        \"pitch\": tf.cast(parsed_features['pitch'].values, tf.float32),\n",
        "        \"pitch_confidence\": tf.cast(parsed_features['pitch_confidence'].values, tf.float32),\n",
        "    }\n",
        "    data = tf.io.decode_raw(parsed_features['data'].values[0], tf.int16)\n",
        "    decoded_features.update({\"data\": data})\n",
        "    return decoded_features\n",
        "\n",
        "\n",
        "def _mix_noisy_speech(speech, noise):\n",
        "    speech_pow = tf.math.reduce_euclidean_norm(speech)\n",
        "    noise_pow = tf.math.reduce_euclidean_norm(noise)\n",
        "\n",
        "    min_SNR = SNR_RANGE[0]\n",
        "    max_SNR = SNR_RANGE[1]\n",
        "    snr_current = 20.0*tf.math.log(speech_pow/noise_pow)/tf.math.log(10.0)\n",
        "    snr_target = tf.random.uniform((),minval=min_SNR,maxval=max_SNR)\n",
        "\n",
        "    noise = noise * tf.math.pow(10.0,(snr_current-snr_target)/20.0)\n",
        "    noisy_speech = speech+noise\n",
        "\n",
        "    return speech, noise, noisy_speech\n",
        "\n",
        "\n",
        "def _interpolate_pitch(pitch,t):\n",
        "    pitches = pitch.numpy()\n",
        "    t = t.numpy()\n",
        "    t_pitch = np.arange(0, len(pitch)) * PITCH_SAMPLING_TIME + PITCH_FRAME_LENGTH / 2\n",
        "    f = scipy.interpolate.interp1d(t_pitch, pitch, 'nearest')\n",
        "    return f(t).astype(np.float32)\n",
        "\n",
        "def convert_hz_to_cent(f,fref=10.0):\n",
        "    return mir_eval.melody.hz2cents(np.array(f), fref)\n",
        "\n",
        "def calc_bin(freq_cent, cents_per_bin = 20, lower_bound_freq=32.7):  \n",
        "    freq_cent = np.squeeze(freq_cent)\n",
        "    lower_bound_freq_cent = mir_eval.melody.hz2cents(np.array([lower_bound_freq]))\n",
        "    bin = (freq_cent - lower_bound_freq_cent) / np.array([cents_per_bin])\n",
        "    return np.clip(bin, 0, 359)\n",
        "\n",
        "def calc_y(f_groundtruth, n_bins = 360):\n",
        "    c_true = calc_bin(f_groundtruth)\n",
        "    return create_bin_vector(c_true)\n",
        "\n",
        "def create_bin_vector(c_true):\n",
        "    cis = np.arange(360)\n",
        "    y = [gaussian_blur(cis, i) for i in c_true]\n",
        "    return np.squeeze(y)\n",
        "    \n",
        "def gaussian_blur(ci, ctrue):\n",
        "    return np.exp(-(ci-ctrue)**2/(2.0*25.0**2))\n",
        "\n",
        "@tf.function\n",
        "def _interpolate_pitch_tf(pitch,t):\n",
        "    y = tf.py_function(_interpolate_pitch,[pitch,t], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "@tf.function\n",
        "def _convert_hz_to_cent(pitch):\n",
        "    y = tf.py_function(convert_hz_to_cent,[pitch], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "@tf.function\n",
        "def _calc_y(pitch_cents):\n",
        "    y = tf.py_function(calc_y,[pitch_cents], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "def _calc_features(speech_data, noise_data):\n",
        "    speech = tf.squeeze(tf.cast(speech_data[\"data\"], tf.float32))\n",
        "    noise = tf.squeeze(tf.cast(noise_data[\"data\"], tf.float32))\n",
        "    speech = speech / tf.int16.max\n",
        "    noise = noise / tf.int16.max\n",
        "\n",
        "    random_start_idx = int(tf.round(tf.random.uniform([], maxval=(\n",
        "             tf.cast(len(noise), tf.float32) - SAMPLE_LENGTH * FS - PITCH_SAMPLING_TIME))))\n",
        "    noise = noise[random_start_idx:random_start_idx + SAMPLE_LENGTH * FS]\n",
        "\n",
        "    random_start_idx = int(tf.round(tf.random.uniform([], minval=161, maxval=(\n",
        "            tf.cast(len(speech), tf.float32) - SAMPLE_LENGTH * FS - 161))))\n",
        "    speech = speech[random_start_idx:random_start_idx + SAMPLE_LENGTH * FS]   \n",
        "\n",
        "    #SNR_range = SNR_RANGE\n",
        "    frame_length = FRAME_LENGTH\n",
        "    frame_step = FRAME_STEP\n",
        "    speech, noise, noisy = _mix_noisy_speech(speech, noise)\n",
        "\n",
        "    random_gain = tf.math.exp(\n",
        "        tf.random.uniform([], minval=tf.math.log(MIN_RAND_GAIN), maxval=tf.math.log(MAX_RAND_GAIN)))\n",
        "    noisy = random_gain * noisy\n",
        "\n",
        "    noisy_frames = tf.signal.frame(noisy, frame_length, frame_step)\n",
        "    speech_frames = tf.signal.frame(speech, frame_length, frame_step)\n",
        "    noisy_frames = tf.squeeze(noisy_frames)\n",
        "    speech_frames = tf.squeeze(speech_frames)\n",
        "    #noisy_stft = tf.signal.stft(noisy,frame_length,frame_step)\n",
        "    frame_times = random_start_idx / FS + tf.range(0, NUM_FRAMES) * frame_step / FS + frame_length / FS\n",
        "    \n",
        "    pitch = tf.squeeze(speech_data[\"pitch\"])    \n",
        "    pitch_confidence = tf.squeeze(speech_data[\"pitch_confidence\"])\n",
        "    #pitch = tf.where(pitch_confidence>config['pitch_confidence_threshold'],pitch,0)\n",
        "    pitch_interpolated = _interpolate_pitch_tf(pitch, frame_times)\n",
        "    pitch_interpolated_cents = _convert_hz_to_cent(pitch_interpolated)\n",
        "    pitch_bins = _calc_y(pitch_interpolated_cents)\n",
        "    return noisy_frames, pitch_bins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0TKt5eSs0SF"
      },
      "source": [
        "## Provide Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiU-wMWPs2dZ"
      },
      "source": [
        "def get_training_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_TR_DIR, file) for file in os.listdir(SPEECH_DATA_TR_DIR)])\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_TR_DIR, file) for file in os.listdir(NOISE_DATA_TR_DIR)])\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "    return dataset_features\n",
        "\n",
        "\n",
        "def get_validation_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_CV_DIR, file) for file in os.listdir(SPEECH_DATA_CV_DIR)])\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_CV_DIR, file) for file in os.listdir(NOISE_DATA_CV_DIR)])\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset_features\n",
        "\n",
        "\n",
        "def get_test_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_TT_DIR, file) for file in os.listdir(SPEECH_DATA_TT_DIR)])\n",
        "    # speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(10).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_TT_DIR, file) for file in os.listdir(NOISE_DATA_TT_DIR)])\n",
        "    # noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(10).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdIXYFTDoN1j"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKx4gLkKjwQ"
      },
      "source": [
        "## DEEP-F0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPygi-4sKooq"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling2D, Dropout, Permute, Flatten, Dense, Add, ReLU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def resblock(x, filters, kernelsize, dilationrate, name):\n",
        "    fx = Conv2D(filters, (kernelsize, 1), dilation_rate=(dilationrate, 1), padding='same',\n",
        "                   activation='relu', name=\"dilation-conv%d\" % name)(x)\n",
        "    fx = BatchNormalization()(fx)\n",
        "    fx = Conv2D(filters, 1, padding='same')(fx)\n",
        "    out = Add()([x,fx])\n",
        "    out = ReLU()(out)\n",
        "    return out\n",
        "\n",
        "def get_model_deepf0():\n",
        "    layers = 1\n",
        "    filters = 128\n",
        "    width = 64\n",
        "    strides = (16, 1)\n",
        "    dilation_rate = 8\n",
        "\n",
        "    x = Input(shape=(FRAME_LENGTH,), name='input', dtype='float32')\n",
        "    y = Reshape(target_shape=(FRAME_LENGTH, 1, 1), name='input-reshape')(x)\n",
        "\n",
        "\n",
        "    y = Conv2D(filters, (width, 1), strides=strides, padding='same',\n",
        "                activation='relu')(y)\n",
        "    for i in range(4):\n",
        "        y = resblock(y, filters, 64, dilation_rate, name=i)\n",
        "    y = AveragePooling2D(pool_size=(2, 1), strides=None, padding='valid',\n",
        "                        name=\"conv1d-avgpool\")(y)\n",
        "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
        "    y = Flatten(name=\"flatten\")(y)\n",
        "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
        "   \n",
        "\n",
        "    model = Model(inputs=x, outputs=y)\n",
        "    model.compile('adam', 'binary_crossentropy', metrics=['mse', 'mae'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-SpsYhaCNV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g6YW7mUtA64"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uU1pyCDPm8C7"
      },
      "source": [
        "dataset_training = get_training_data()\n",
        "dataset_validation = get_validation_data()\n",
        "dataset_test = get_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dzEiCs_tDcJ"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poP21a6TnLHO"
      },
      "source": [
        "model = get_model_deepf0()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33_wEPK8nNfC"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9FxwYbUXD3_"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbVgnmcitIjt"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHZ4eMqMnchd",
        "collapsed": true
      },
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/BA_2021/deepf0/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPzG257AP4uR"
      },
      "source": [
        "# USE IF IT IS INITIAL TRAINING\n",
        "\n",
        "MODEL_USED = 'deepf0'\n",
        "LOG_DIR = os.path.join(_DATA_DIR, MODEL_USED, 'logs', TIMESTAMP + '_1024_512_100_Epochs')\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "CHECKPOINT_DIR = os.path.join(_DATA_DIR, MODEL_USED, 'checkpoints', TIMESTAMP + '_1024_512_100_Epochs')\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn0JRWrccNJH"
      },
      "source": [
        "# JUST USE IF CONTINUING TRAINING\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/BA_2021/crepe/checkpoints/20210427-145400'\n",
        "LOGDIR = '/content/drive/MyDrive/BA_2021/crepe/logs/20210427-145400'\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/crepe/checkpoints', '20210427-145400', '50-2063.93.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OMUc5cRQnSNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8d0e74-ce2a-4e85-a173-5a08f0e5320c"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(LOG_DIR, histogram_freq=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(CHECKPOINT_DIR,'{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
        "early_stopping =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=32, mode='min')\n",
        "\n",
        "callbacks = [checkpoint, tensorboard_callback, early_stopping]\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_training,\n",
        "    steps_per_epoch=500,\n",
        "    epochs=100,\n",
        "    # initial_epoch=30,\n",
        "    verbose = 1,\n",
        "    validation_data = dataset_validation,\n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    callbacks = callbacks)\n",
        "    \n",
        "loss = model.evaluate(dataset_test, steps=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "500/500 [==============================] - 15s 28ms/step - loss: 0.2147 - mse: 0.0304 - mae: 0.0819 - val_loss: 0.3367 - val_mse: 0.0489 - val_mae: 0.0858\n",
            "Epoch 2/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1952 - mse: 0.0241 - mae: 0.0672 - val_loss: 0.2099 - val_mse: 0.0284 - val_mae: 0.0613\n",
            "Epoch 3/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1839 - mse: 0.0210 - mae: 0.0589 - val_loss: 0.2195 - val_mse: 0.0329 - val_mae: 0.0876\n",
            "Epoch 4/100\n",
            "500/500 [==============================] - 13s 27ms/step - loss: 0.1877 - mse: 0.0223 - mae: 0.0624 - val_loss: 0.2048 - val_mse: 0.0266 - val_mae: 0.0632\n",
            "Epoch 5/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1870 - mse: 0.0218 - mae: 0.0615 - val_loss: 0.1779 - val_mse: 0.0199 - val_mae: 0.0615\n",
            "Epoch 6/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1852 - mse: 0.0210 - mae: 0.0592 - val_loss: 0.2070 - val_mse: 0.0285 - val_mae: 0.0728\n",
            "Epoch 7/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1808 - mse: 0.0200 - mae: 0.0569 - val_loss: 0.2005 - val_mse: 0.0244 - val_mae: 0.0618\n",
            "Epoch 8/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1693 - mse: 0.0175 - mae: 0.0499 - val_loss: 0.1838 - val_mse: 0.0209 - val_mae: 0.0609\n",
            "Epoch 9/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1683 - mse: 0.0170 - mae: 0.0489 - val_loss: 0.2061 - val_mse: 0.0259 - val_mae: 0.0674\n",
            "Epoch 10/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1671 - mse: 0.0170 - mae: 0.0485 - val_loss: 0.1822 - val_mse: 0.0223 - val_mae: 0.0643\n",
            "Epoch 11/100\n",
            "500/500 [==============================] - 13s 27ms/step - loss: 0.1587 - mse: 0.0147 - mae: 0.0424 - val_loss: 0.1871 - val_mse: 0.0239 - val_mae: 0.0635\n",
            "Epoch 12/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1705 - mse: 0.0176 - mae: 0.0500 - val_loss: 0.1785 - val_mse: 0.0205 - val_mae: 0.0607\n",
            "Epoch 13/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1714 - mse: 0.0175 - mae: 0.0496 - val_loss: 0.1881 - val_mse: 0.0243 - val_mae: 0.0614\n",
            "Epoch 14/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1653 - mse: 0.0158 - mae: 0.0451 - val_loss: 0.1613 - val_mse: 0.0165 - val_mae: 0.0531\n",
            "Epoch 15/100\n",
            "500/500 [==============================] - 13s 27ms/step - loss: 0.1689 - mse: 0.0165 - mae: 0.0467 - val_loss: 0.1695 - val_mse: 0.0176 - val_mae: 0.0523\n",
            "Epoch 16/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1749 - mse: 0.0182 - mae: 0.0511 - val_loss: 0.1636 - val_mse: 0.0160 - val_mae: 0.0505\n",
            "Epoch 17/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1685 - mse: 0.0165 - mae: 0.0467 - val_loss: 0.2008 - val_mse: 0.0274 - val_mae: 0.0673\n",
            "Epoch 18/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1671 - mse: 0.0158 - mae: 0.0452 - val_loss: 0.1759 - val_mse: 0.0192 - val_mae: 0.0508\n",
            "Epoch 19/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1651 - mse: 0.0157 - mae: 0.0444 - val_loss: 0.1702 - val_mse: 0.0178 - val_mae: 0.0521\n",
            "Epoch 20/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1669 - mse: 0.0160 - mae: 0.0456 - val_loss: 0.1733 - val_mse: 0.0182 - val_mae: 0.0475\n",
            "Epoch 21/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1718 - mse: 0.0176 - mae: 0.0497 - val_loss: 0.2175 - val_mse: 0.0308 - val_mae: 0.0717\n",
            "Epoch 22/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1663 - mse: 0.0160 - mae: 0.0458 - val_loss: 0.1652 - val_mse: 0.0167 - val_mae: 0.0483\n",
            "Epoch 23/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1657 - mse: 0.0160 - mae: 0.0456 - val_loss: 0.1653 - val_mse: 0.0175 - val_mae: 0.0475\n",
            "Epoch 24/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1669 - mse: 0.0163 - mae: 0.0462 - val_loss: 0.1796 - val_mse: 0.0184 - val_mae: 0.0512\n",
            "Epoch 25/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1711 - mse: 0.0173 - mae: 0.0489 - val_loss: 0.1916 - val_mse: 0.0248 - val_mae: 0.0716\n",
            "Epoch 26/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1585 - mse: 0.0139 - mae: 0.0402 - val_loss: 0.1711 - val_mse: 0.0192 - val_mae: 0.0534\n",
            "Epoch 27/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1534 - mse: 0.0129 - mae: 0.0376 - val_loss: 0.1709 - val_mse: 0.0168 - val_mae: 0.0483\n",
            "Epoch 28/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1571 - mse: 0.0139 - mae: 0.0399 - val_loss: 0.1589 - val_mse: 0.0137 - val_mae: 0.0364\n",
            "Epoch 29/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1622 - mse: 0.0150 - mae: 0.0428 - val_loss: 0.1674 - val_mse: 0.0175 - val_mae: 0.0539\n",
            "Epoch 30/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1694 - mse: 0.0163 - mae: 0.0465 - val_loss: 0.1518 - val_mse: 0.0118 - val_mae: 0.0357\n",
            "Epoch 31/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1703 - mse: 0.0165 - mae: 0.0466 - val_loss: 0.1521 - val_mse: 0.0132 - val_mae: 0.0363\n",
            "Epoch 32/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1717 - mse: 0.0173 - mae: 0.0483 - val_loss: 0.1629 - val_mse: 0.0168 - val_mae: 0.0558\n",
            "Epoch 33/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1704 - mse: 0.0168 - mae: 0.0471 - val_loss: 0.2017 - val_mse: 0.0238 - val_mae: 0.0585\n",
            "Epoch 34/100\n",
            "500/500 [==============================] - 13s 27ms/step - loss: 0.1722 - mse: 0.0169 - mae: 0.0476 - val_loss: 0.1795 - val_mse: 0.0202 - val_mae: 0.0556\n",
            "Epoch 35/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1609 - mse: 0.0150 - mae: 0.0421 - val_loss: 0.1727 - val_mse: 0.0176 - val_mae: 0.0494\n",
            "Epoch 36/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1612 - mse: 0.0151 - mae: 0.0426 - val_loss: 0.1755 - val_mse: 0.0187 - val_mae: 0.0595\n",
            "Epoch 37/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1672 - mse: 0.0160 - mae: 0.0453 - val_loss: 0.1780 - val_mse: 0.0195 - val_mae: 0.0545\n",
            "Epoch 38/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1656 - mse: 0.0155 - mae: 0.0443 - val_loss: 0.1516 - val_mse: 0.0130 - val_mae: 0.0417\n",
            "Epoch 39/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1681 - mse: 0.0163 - mae: 0.0461 - val_loss: 0.1912 - val_mse: 0.0228 - val_mae: 0.0708\n",
            "Epoch 40/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1645 - mse: 0.0154 - mae: 0.0441 - val_loss: 0.1580 - val_mse: 0.0146 - val_mae: 0.0446\n",
            "Epoch 41/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1578 - mse: 0.0139 - mae: 0.0397 - val_loss: 0.1557 - val_mse: 0.0138 - val_mae: 0.0414\n",
            "Epoch 42/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1590 - mse: 0.0144 - mae: 0.0410 - val_loss: 0.1932 - val_mse: 0.0219 - val_mae: 0.0557\n",
            "Epoch 43/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1513 - mse: 0.0127 - mae: 0.0362 - val_loss: 0.1968 - val_mse: 0.0254 - val_mae: 0.0640\n",
            "Epoch 44/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1584 - mse: 0.0143 - mae: 0.0408 - val_loss: 0.1473 - val_mse: 0.0120 - val_mae: 0.0416\n",
            "Epoch 45/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1596 - mse: 0.0142 - mae: 0.0408 - val_loss: 0.1795 - val_mse: 0.0206 - val_mae: 0.0441\n",
            "Epoch 46/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1622 - mse: 0.0148 - mae: 0.0420 - val_loss: 0.1445 - val_mse: 0.0116 - val_mae: 0.0335\n",
            "Epoch 47/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1641 - mse: 0.0151 - mae: 0.0430 - val_loss: 0.2045 - val_mse: 0.0272 - val_mae: 0.0643\n",
            "Epoch 48/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1613 - mse: 0.0143 - mae: 0.0407 - val_loss: 0.1821 - val_mse: 0.0189 - val_mae: 0.0523\n",
            "Epoch 49/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1691 - mse: 0.0165 - mae: 0.0462 - val_loss: 0.1639 - val_mse: 0.0163 - val_mae: 0.0448\n",
            "Epoch 50/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1737 - mse: 0.0176 - mae: 0.0491 - val_loss: 0.1794 - val_mse: 0.0183 - val_mae: 0.0522\n",
            "Epoch 51/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1599 - mse: 0.0142 - mae: 0.0403 - val_loss: 0.1650 - val_mse: 0.0169 - val_mae: 0.0473\n",
            "Epoch 52/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1613 - mse: 0.0146 - mae: 0.0413 - val_loss: 0.1786 - val_mse: 0.0174 - val_mae: 0.0590\n",
            "Epoch 53/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1681 - mse: 0.0165 - mae: 0.0462 - val_loss: 0.1705 - val_mse: 0.0177 - val_mae: 0.0474\n",
            "Epoch 54/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1726 - mse: 0.0174 - mae: 0.0488 - val_loss: 0.1670 - val_mse: 0.0183 - val_mae: 0.0515\n",
            "Epoch 55/100\n",
            "500/500 [==============================] - 13s 27ms/step - loss: 0.1664 - mse: 0.0158 - mae: 0.0447 - val_loss: 0.1684 - val_mse: 0.0155 - val_mae: 0.0455\n",
            "Epoch 56/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1606 - mse: 0.0140 - mae: 0.0403 - val_loss: 0.1846 - val_mse: 0.0216 - val_mae: 0.0528\n",
            "Epoch 57/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1592 - mse: 0.0143 - mae: 0.0405 - val_loss: 0.1716 - val_mse: 0.0162 - val_mae: 0.0430\n",
            "Epoch 58/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1586 - mse: 0.0144 - mae: 0.0407 - val_loss: 0.1679 - val_mse: 0.0181 - val_mae: 0.0502\n",
            "Epoch 59/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1577 - mse: 0.0142 - mae: 0.0402 - val_loss: 0.1630 - val_mse: 0.0161 - val_mae: 0.0425\n",
            "Epoch 60/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1563 - mse: 0.0140 - mae: 0.0395 - val_loss: 0.1565 - val_mse: 0.0144 - val_mae: 0.0398\n",
            "Epoch 61/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1564 - mse: 0.0134 - mae: 0.0385 - val_loss: 0.2000 - val_mse: 0.0233 - val_mae: 0.0517\n",
            "Epoch 62/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1609 - mse: 0.0143 - mae: 0.0404 - val_loss: 0.1724 - val_mse: 0.0182 - val_mae: 0.0500\n",
            "Epoch 63/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1583 - mse: 0.0137 - mae: 0.0391 - val_loss: 0.1740 - val_mse: 0.0176 - val_mae: 0.0445\n",
            "Epoch 64/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1636 - mse: 0.0149 - mae: 0.0420 - val_loss: 0.1650 - val_mse: 0.0156 - val_mae: 0.0445\n",
            "Epoch 65/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1732 - mse: 0.0175 - mae: 0.0486 - val_loss: 0.1733 - val_mse: 0.0194 - val_mae: 0.0475\n",
            "Epoch 66/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1679 - mse: 0.0157 - mae: 0.0441 - val_loss: 0.1775 - val_mse: 0.0187 - val_mae: 0.0530\n",
            "Epoch 67/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1637 - mse: 0.0149 - mae: 0.0422 - val_loss: 0.1752 - val_mse: 0.0186 - val_mae: 0.0523\n",
            "Epoch 68/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1617 - mse: 0.0148 - mae: 0.0419 - val_loss: 0.1583 - val_mse: 0.0132 - val_mae: 0.0419\n",
            "Epoch 69/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1653 - mse: 0.0152 - mae: 0.0431 - val_loss: 0.1533 - val_mse: 0.0131 - val_mae: 0.0431\n",
            "Epoch 70/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1656 - mse: 0.0155 - mae: 0.0438 - val_loss: 0.1502 - val_mse: 0.0114 - val_mae: 0.0408\n",
            "Epoch 71/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1663 - mse: 0.0155 - mae: 0.0441 - val_loss: 0.1603 - val_mse: 0.0160 - val_mae: 0.0479\n",
            "Epoch 72/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1621 - mse: 0.0144 - mae: 0.0413 - val_loss: 0.1920 - val_mse: 0.0230 - val_mae: 0.0688\n",
            "Epoch 73/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1656 - mse: 0.0158 - mae: 0.0447 - val_loss: 0.1741 - val_mse: 0.0184 - val_mae: 0.0514\n",
            "Epoch 74/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1582 - mse: 0.0140 - mae: 0.0396 - val_loss: 0.1752 - val_mse: 0.0185 - val_mae: 0.0518\n",
            "Epoch 75/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1623 - mse: 0.0150 - mae: 0.0423 - val_loss: 0.1619 - val_mse: 0.0172 - val_mae: 0.0420\n",
            "Epoch 76/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1554 - mse: 0.0137 - mae: 0.0389 - val_loss: 0.1575 - val_mse: 0.0145 - val_mae: 0.0435\n",
            "Epoch 77/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1564 - mse: 0.0136 - mae: 0.0388 - val_loss: 0.1851 - val_mse: 0.0236 - val_mae: 0.0561\n",
            "Epoch 78/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1556 - mse: 0.0130 - mae: 0.0376 - val_loss: 0.1440 - val_mse: 0.0112 - val_mae: 0.0341\n",
            "Epoch 79/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1647 - mse: 0.0150 - mae: 0.0425 - val_loss: 0.1734 - val_mse: 0.0177 - val_mae: 0.0449\n",
            "Epoch 80/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1583 - mse: 0.0136 - mae: 0.0384 - val_loss: 0.1661 - val_mse: 0.0148 - val_mae: 0.0426\n",
            "Epoch 81/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1612 - mse: 0.0142 - mae: 0.0400 - val_loss: 0.1722 - val_mse: 0.0170 - val_mae: 0.0463\n",
            "Epoch 82/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1668 - mse: 0.0154 - mae: 0.0435 - val_loss: 0.1481 - val_mse: 0.0127 - val_mae: 0.0357\n",
            "Epoch 83/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1640 - mse: 0.0145 - mae: 0.0410 - val_loss: 0.1865 - val_mse: 0.0216 - val_mae: 0.0502\n",
            "Epoch 84/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1571 - mse: 0.0133 - mae: 0.0375 - val_loss: 0.1727 - val_mse: 0.0179 - val_mae: 0.0450\n",
            "Epoch 85/100\n",
            "500/500 [==============================] - 13s 27ms/step - loss: 0.1596 - mse: 0.0141 - mae: 0.0398 - val_loss: 0.1706 - val_mse: 0.0177 - val_mae: 0.0460\n",
            "Epoch 86/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1642 - mse: 0.0156 - mae: 0.0435 - val_loss: 0.1667 - val_mse: 0.0156 - val_mae: 0.0470\n",
            "Epoch 87/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1651 - mse: 0.0156 - mae: 0.0437 - val_loss: 0.1665 - val_mse: 0.0165 - val_mae: 0.0439\n",
            "Epoch 88/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1647 - mse: 0.0152 - mae: 0.0433 - val_loss: 0.1866 - val_mse: 0.0223 - val_mae: 0.0543\n",
            "Epoch 89/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1595 - mse: 0.0143 - mae: 0.0404 - val_loss: 0.1550 - val_mse: 0.0144 - val_mae: 0.0408\n",
            "Epoch 90/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1608 - mse: 0.0148 - mae: 0.0416 - val_loss: 0.1554 - val_mse: 0.0147 - val_mae: 0.0424\n",
            "Epoch 91/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1572 - mse: 0.0140 - mae: 0.0395 - val_loss: 0.1421 - val_mse: 0.0112 - val_mae: 0.0336\n",
            "Epoch 92/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1588 - mse: 0.0142 - mae: 0.0402 - val_loss: 0.1642 - val_mse: 0.0185 - val_mae: 0.0491\n",
            "Epoch 93/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1524 - mse: 0.0126 - mae: 0.0361 - val_loss: 0.1744 - val_mse: 0.0181 - val_mae: 0.0435\n",
            "Epoch 94/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1555 - mse: 0.0132 - mae: 0.0378 - val_loss: 0.2195 - val_mse: 0.0267 - val_mae: 0.0517\n",
            "Epoch 95/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1626 - mse: 0.0148 - mae: 0.0419 - val_loss: 0.1763 - val_mse: 0.0186 - val_mae: 0.0555\n",
            "Epoch 96/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 0.1618 - mse: 0.0144 - mae: 0.0410 - val_loss: 0.1771 - val_mse: 0.0200 - val_mae: 0.0512\n",
            "Epoch 97/100\n",
            "500/500 [==============================] - 13s 26ms/step - loss: 0.1636 - mse: 0.0150 - mae: 0.0421 - val_loss: 0.1721 - val_mse: 0.0169 - val_mae: 0.0435\n",
            "Epoch 98/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1635 - mse: 0.0150 - mae: 0.0420 - val_loss: 0.1555 - val_mse: 0.0144 - val_mae: 0.0424\n",
            "Epoch 99/100\n",
            "500/500 [==============================] - 14s 27ms/step - loss: 0.1623 - mse: 0.0145 - mae: 0.0410 - val_loss: 0.1744 - val_mse: 0.0191 - val_mae: 0.0492\n",
            "Epoch 100/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.1623 - mse: 0.0147 - mae: 0.0413 - val_loss: 0.1562 - val_mse: 0.0146 - val_mae: 0.0432\n",
            "70/70 [==============================] - 7s 20ms/step - loss: 0.1740 - mse: 0.0189 - mae: 0.0488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVozVO6-vtO3"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msEYwMhNddcK"
      },
      "source": [
        "# 1024 / 512\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210502-204318_1024_512_100_Epochs', '87-0.16.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb4YnOFA_dqZ"
      },
      "source": [
        "# 512 / 256\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210503-142217_512_256_100_Epochs', '48-0.22.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM3NbC0mkbmg"
      },
      "source": [
        "# 256 / 128\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210503-193625_256_128_100_Epochs', '91-0.22.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrZRWxGWRGLu"
      },
      "source": [
        "# 2048 / 1024\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210520-102227_2048_1024_100_Epochs', '47-0.18.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmvJr2v9lCmz"
      },
      "source": [
        "from rt_pie_lib import converters\n",
        "def prediction():\n",
        "    predicted_c = []\n",
        "    true_c = []\n",
        "    inp_vector = []\n",
        "    predicted_vector = []\n",
        "    for inp, outp in dataset_test:\n",
        "        predicted = model.predict(inp)\n",
        "        predicted_vector.append(predicted)\n",
        "        inp_vector.append(outp) \n",
        "        true_cents = converters.convert_bin_to_local_average_cents(outp)\n",
        "        true_c.append(true_cents)\n",
        "        #predicted_cents = convert_bin_to_local_average_cents_lowest_maxima(np.squeeze(predicted))\n",
        "        predicted_cents = converters.convert_bin_to_local_average_cents(np.squeeze(predicted))\n",
        "        predicted_c.append(predicted_cents)\n",
        "\n",
        "    predicted_vector = np.reshape(np.array(predicted_vector), ((len(predicted_vector) * len(predicted_vector[0]), 360)))\n",
        "    inp_vector = np.reshape(np.array(inp_vector), ((len(inp_vector) * len(inp_vector[0]), 360)))\n",
        "    \n",
        "    true_c = np.reshape(np.array(true_c), (1, (len(true_c)*len(true_c[0]))))\n",
        "    true_c = np.squeeze(true_c)\n",
        "    true_hz = converters.convert_cent_to_hz(true_c)\n",
        "    predicted_c = np.reshape(np.array(predicted_c), (1, (len(predicted_c)*len(predicted_c[0]))))\n",
        "    predicted_c = np.squeeze(predicted_c)\n",
        "    predicted_hz = converters.convert_cent_to_hz(predicted_c)\n",
        "    diff = true_hz - predicted_hz\n",
        "    return predicted_hz, true_hz, true_c, predicted_c, diff, inp_vector, predicted_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh3HIl3f7ONH"
      },
      "source": [
        "predicted_hz, true_hz, true_cent, predicted_cent, diff, inp_vector, predicted_vector = prediction()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4LPGpFaYwvU"
      },
      "source": [
        "### Prediction Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUs_jXsA-8h5"
      },
      "source": [
        "## Hertz filter\n",
        "combined = zip(true_hz, predicted_hz)\n",
        "filtered = combined\n",
        "# filtered = [x for x in list(combined) if x[1] > 60 and x[1] < 400]# and x[1] > 60 and x[1] < 400]\n",
        "filtered_unzipped = np.array(list(zip(*filtered)))\n",
        "diff_filtered = filtered_unzipped[0] - filtered_unzipped[1]\n",
        "print(len(diff), len(diff_filtered))\n",
        "## Cent filter\n",
        "combined_cent = zip(true_cent, predicted_cent)\n",
        "filtered_cent = [x for x in list(combined_cent) if x[0] > 3101.95500087 and x[1] > 3101.95500087]\n",
        "filtered_c_unzipped = np.array(list(zip(*filtered_cent)))\n",
        "diff_filtered_cent = filtered_c_unzipped[0] - filtered_c_unzipped[1]\n",
        "\n",
        "\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "from rt_pie_lib import metrics\n",
        "hz_metrics = metrics.get_hz_metrics(filtered_unzipped[0], filtered_unzipped[1], print_output=True)\n",
        "rpa_cent = metrics.raw_pitch_accuracy_cent(filtered_c_unzipped[0], filtered_c_unzipped[1])\n",
        "print(rpa_cent)\n",
        "hist = histogram(diff_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAMZz2UdkylA"
      },
      "source": [
        "## Hz Values with factor 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNSM6rADKWzQ"
      },
      "source": [
        "np.set_printoptions(threshold=360)\n",
        "counter = 0\n",
        "trues = np.array([])\n",
        "preds = np.array([])\n",
        "for i in range(len(diff)):\n",
        "    if diff[i] > -50 and diff[i] < 50  and true_hz[i] > 60:\n",
        "        plt.figure(i)\n",
        "        plt.plot(inp_vector[i], 'r')\n",
        "        plt.plot(predicted_vector[i], 'b')\n",
        "        plt.plot(np.argmax(inp_vector[i]),np.max(inp_vector[i]),'x')\n",
        "        plt.plot(np.argmax(predicted_vector[i]),np.max(predicted_vector[i]),'x')\n",
        "        plt.text(np.argmax(inp_vector[i])+10,np.max(inp_vector[i]),f'max={np.max(inp_vector[i]):.1f} @ bin {np.argmax(inp_vector[i])}')\n",
        "        plt.text(np.argmax(predicted_vector[i])+10,np.max(predicted_vector[i]),f'max={np.max(predicted_vector[i]):.1f} @ bin {np.argmax(predicted_vector[i])}')\n",
        "        plt.show()\n",
        "\n",
        "        \n",
        "        print('True Hz: ', true_hz[i], 'Predicted Hz: ',predicted_hz[i])\n",
        "        #trues = np.append(trues, filtered_unzipped[0][i])\n",
        "        #preds = np.append(preds, filtered_unzipped[1][i])\n",
        "        # counter += 1\n",
        "        # if counter == 60:\n",
        "        #     break   \n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRJb_B_ryvm5",
        "outputId": "08b9c6da-043a-47e4-8d8a-5ee05f369f17"
      },
      "source": [
        "divided = np.divide(preds, trues)\n",
        "np.mean(divided)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.621543653541332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-AXsfqli1Nl"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr6y9h0_cCRJ"
      },
      "source": [
        "plt.rcParams.update({'font.size': 22})\n",
        "from rt_pie_lib import metrics\n",
        "hz_metrics = metrics.get_hz_metrics(filtered_unzipped[0], filtered_unzipped[1], print_output=True)\n",
        "rpa_cent = metrics.raw_pitch_accuracy_cent(filtered_c_unzipped[0], filtered_c_unzipped[1])\n",
        "print(rpa_cent)\n",
        "hist = histogram(diff_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ2347buZISB"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qtgyIEBiJz7"
      },
      "source": [
        "def histogram(diff):  \n",
        "    n_bins = 250\n",
        "    x = diff\n",
        "    y = true_hz\n",
        "\n",
        "    plt.figure(figsize=[16,9])\n",
        "    plt.hist(x, bins=n_bins)\n",
        "    #plt.xlim([-200, 200])\n",
        "    plt.ylim([0, 30000])\n",
        "    plt.axvline(np.median(x), color='k', linestyle='dashed', linewidth=2, label='MED')\n",
        "    plt.axvline(np.mean(x), color='k', linestyle='solid', linewidth=2, label='MEAN')\n",
        "    plt.axvline(np.quantile(x, 0.05), color='k', linestyle='dotted', linewidth=2, label='5% quantile')\n",
        "    plt.axvline(np.quantile(x, 0.95), color='k', linestyle='dashdot', linewidth=2, label='95% quantile')\n",
        "    plt.xlabel(\"Error in Hertz\")\n",
        "    plt.ylabel(\"Number of Errors\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# histo = histogram(diff)\n",
        "# histo_true = histogram([x[0] - x[1] for x in filtered])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}