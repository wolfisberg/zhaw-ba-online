{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP-F0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMLbGy+nud0XFQbXsyS2MBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfisberg/zhaw-ba-online/blob/main/deepf0/DEEP_F0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU5ZvlGglGDd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7h2rENnSN3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5715bcd5-b1f5-4962-e3fb-660ff8ae15b5"
      },
      "source": [
        "!pip install mir_eval"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mir_eval in /usr/local/lib/python3.7/dist-packages (0.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir_eval) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q42BY8BPSoL",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d878b01-85da-45b9-ff46-ff9d4f40e56f"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.interpolate\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import datetime\n",
        "import mir_eval\n",
        "import math\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8nh41ellB7K"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWZIc9-WSItM",
        "collapsed": true
      },
      "source": [
        "# Audio\n",
        "SNR_RANGE = (-5.0,20.0) #dB\n",
        "FRAME_LENGTH = 1024\n",
        "FRAME_STEP = 512\n",
        "MIN_RAND_GAIN = 0.05\n",
        "MAX_RAND_GAIN = 1.1\n",
        "SAMPLE_LENGTH = 3 #shorter than shortest noise/speech sample\n",
        "FS = 16000\n",
        "PITCH_SAMPLING_TIME = 0.01 # s\n",
        "PITCH_FRAME_LENGTH = 0.032 # s\n",
        "\n",
        "\n",
        "# Data\n",
        "BATCH_SIZE = 64\n",
        "NUM_FRAMES = 1 + (FS * SAMPLE_LENGTH - FRAME_LENGTH) // FRAME_STEP\n",
        "# NUM_FRAMES = 1\n",
        "\n",
        "# Training\n",
        "STEPS_PER_EPOCH = 500\n",
        "EPOCHS = 100\n",
        "VALIDATION_STEPS = 5\n",
        "\n",
        "\n",
        "# Directories\n",
        "_DATA_DIR = os.path.join('/content/drive/MyDrive/BA_2021/')\n",
        "_TFRECORDS_DIR = os.path.join(_DATA_DIR, 'tfrecords')\n",
        "\n",
        "SPEECH_DATA_TR_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'tr')\n",
        "NOISE_DATA_TR_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'tr')\n",
        "SPEECH_DATA_CV_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'cv')\n",
        "NOISE_DATA_CV_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'cv')\n",
        "SPEECH_DATA_TT_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'tt')\n",
        "NOISE_DATA_TT_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'tt')\n",
        "\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "# Misc\n",
        "SEED = 2\n",
        "\n",
        "\n",
        "# Parsing\n",
        "PARSING_CONFIG_NOISE = {\n",
        "    'data': tf.io.VarLenFeature(tf.string),\n",
        "    'data_sampling_rate': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_num_channels': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_width': tf.io.VarLenFeature(tf.int64),\n",
        "}\n",
        "\n",
        "PARSING_CONFIG_SPEECH = {\n",
        "    'data': tf.io.VarLenFeature(tf.string),\n",
        "    'data_sampling_rate': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_num_channels': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_width': tf.io.VarLenFeature(tf.int64),\n",
        "    'pitch': tf.io.VarLenFeature(tf.float32),\n",
        "    'pitch_confidence': tf.io.VarLenFeature(tf.float32),\n",
        "}\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DQaQ8neunsZ"
      },
      "source": [
        "print(NOISE_DATA_TR_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-kwHYrpmCgl"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmb9w4ACwoUi"
      },
      "source": [
        "## Copy Data to Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYKlRsmDwsKM"
      },
      "source": [
        "DATA_DIR_LOCAL = '/content/data'\n",
        "\n",
        "if not os.path.exists(DATA_DIR_LOCAL):\n",
        "    os.mkdir(DATA_DIR_LOCAL)\n",
        "    \n",
        "    RECORD_DIR_LOCAL = os.path.join(DATA_DIR_LOCAL, 'tfrecords')\n",
        "    shutil.copytree(_TFRECORDS_DIR, RECORD_DIR_LOCAL)\n",
        "\n",
        "\n",
        "_TFRECORDS_DIR = os.path.join(DATA_DIR_LOCAL, 'tfrecords')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awsl19RfyOii"
      },
      "source": [
        "print(_TFRECORDS_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlkFt3Nvsqn-"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhwJchGumFSo"
      },
      "source": [
        "def _parse_noise_record(serialized_example):\n",
        "    parsed_features = tf.io.parse_single_example(serialized_example, features=PARSING_CONFIG_NOISE)\n",
        "    decoded_features = {\n",
        "        \"data_num_channels\": tf.cast(parsed_features[\"data_num_channels\"].values[0], tf.int32),\n",
        "        \"data_sampling_rate\": tf.cast(parsed_features[\"data_sampling_rate\"].values[0], tf.int32),\n",
        "        \"data_width\": tf.cast(parsed_features[\"data_width\"].values[0], tf.int32),\n",
        "    }\n",
        "    data = tf.io.decode_raw(parsed_features['data'].values[0], tf.int16)\n",
        "    decoded_features.update({\"data\": data})\n",
        "    return decoded_features\n",
        "\n",
        "\n",
        "def _parse_speech_record(serialized_example):\n",
        "    parsed_features = tf.io.parse_single_example(serialized_example, features=PARSING_CONFIG_SPEECH)\n",
        "    decoded_features = {\n",
        "        \"data_num_channels\": tf.cast(parsed_features[\"data_num_channels\"].values[0], tf.int32),\n",
        "        \"data_sampling_rate\": tf.cast(parsed_features[\"data_sampling_rate\"].values[0], tf.int32),\n",
        "        \"data_width\": tf.cast(parsed_features[\"data_width\"].values[0], tf.int32),\n",
        "        \"pitch\": tf.cast(parsed_features['pitch'].values, tf.float32),\n",
        "        \"pitch_confidence\": tf.cast(parsed_features['pitch_confidence'].values, tf.float32),\n",
        "    }\n",
        "    data = tf.io.decode_raw(parsed_features['data'].values[0], tf.int16)\n",
        "    decoded_features.update({\"data\": data})\n",
        "    return decoded_features\n",
        "\n",
        "\n",
        "def _mix_noisy_speech(speech, noise):\n",
        "    speech_pow = tf.math.reduce_euclidean_norm(speech)\n",
        "    noise_pow = tf.math.reduce_euclidean_norm(noise)\n",
        "\n",
        "    min_SNR = SNR_RANGE[0]\n",
        "    max_SNR = SNR_RANGE[1]\n",
        "    snr_current = 20.0*tf.math.log(speech_pow/noise_pow)/tf.math.log(10.0)\n",
        "    snr_target = tf.random.uniform((),minval=min_SNR,maxval=max_SNR)\n",
        "\n",
        "    noise = noise * tf.math.pow(10.0,(snr_current-snr_target)/20.0)\n",
        "    noisy_speech = speech+noise\n",
        "\n",
        "    return speech, noise, noisy_speech\n",
        "\n",
        "\n",
        "def _interpolate_pitch(pitch,t):\n",
        "    pitches = pitch.numpy()\n",
        "    t = t.numpy()\n",
        "    t_pitch = np.arange(0, len(pitch)) * PITCH_SAMPLING_TIME + PITCH_FRAME_LENGTH / 2\n",
        "    f = scipy.interpolate.interp1d(t_pitch, pitch, 'nearest')\n",
        "    return f(t).astype(np.float32)\n",
        "\n",
        "def convert_hz_to_cent(f,fref=10.0):\n",
        "    return mir_eval.melody.hz2cents(np.array(f), fref)\n",
        "\n",
        "def calc_bin(freq_cent, cents_per_bin = 20, lower_bound_freq=0.0):  \n",
        "    freq_cent = np.squeeze(freq_cent)\n",
        "    #freq_cent = np.reshape(freq_cent, (1, freq_cent[0]*freq_cent[1]))\n",
        "    lower_bound_freq_cent = mir_eval.melody.hz2cents(np.array([lower_bound_freq]))\n",
        "    bin = (freq_cent - lower_bound_freq_cent) / np.array([cents_per_bin])\n",
        "    #print(np.clip(bin, 0, 359))\n",
        "    return np.clip(bin, 0, 359)\n",
        "    #return min(359, max(0, bin))\n",
        "\n",
        "def calc_y(f_groundtruth, n_bins = 360):\n",
        "    c_true = calc_bin(f_groundtruth)\n",
        "    return create_bin_vector(c_true)\n",
        "\n",
        "def create_bin_vector(c_true):\n",
        "    cis = np.arange(360)\n",
        "    # cis = np.tile(cis, (len(c_true), 1))\n",
        "    y = [gaussian_blur(cis, i) for i in c_true]\n",
        "    return np.squeeze(y)\n",
        "    \n",
        "def gaussian_blur(ci, ctrue):\n",
        "    return np.exp(-(ci-ctrue)**2/(2.0*25.0**2))\n",
        "\n",
        "@tf.function\n",
        "def _interpolate_pitch_tf(pitch,t):\n",
        "    y = tf.py_function(_interpolate_pitch,[pitch,t], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "@tf.function\n",
        "def _convert_hz_to_cent(pitch):\n",
        "    y = tf.py_function(convert_hz_to_cent,[pitch], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "@tf.function\n",
        "def _calc_y(pitch_cents):\n",
        "    y = tf.py_function(calc_y,[pitch_cents], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "def _calc_features(speech_data, noise_data):\n",
        "    speech = tf.squeeze(tf.cast(speech_data[\"data\"], tf.float32))\n",
        "    noise = tf.squeeze(tf.cast(noise_data[\"data\"], tf.float32))\n",
        "    speech = speech / tf.int16.max\n",
        "    noise = noise / tf.int16.max\n",
        "\n",
        "    random_start_idx = int(tf.round(tf.random.uniform([], maxval=(\n",
        "             tf.cast(len(noise), tf.float32) - SAMPLE_LENGTH * FS - PITCH_SAMPLING_TIME))))\n",
        "    noise = noise[random_start_idx:random_start_idx + SAMPLE_LENGTH * FS]\n",
        "\n",
        "    random_start_idx = int(tf.round(tf.random.uniform([], minval=161, maxval=(\n",
        "            tf.cast(len(speech), tf.float32) - SAMPLE_LENGTH * FS - 161))))\n",
        "    speech = speech[random_start_idx:random_start_idx + SAMPLE_LENGTH * FS]   \n",
        "\n",
        "    #SNR_range = SNR_RANGE\n",
        "    frame_length = FRAME_LENGTH\n",
        "    frame_step = FRAME_STEP\n",
        "    speech, noise, noisy = _mix_noisy_speech(speech, noise)\n",
        "\n",
        "    random_gain = tf.math.exp(\n",
        "        tf.random.uniform([], minval=tf.math.log(MIN_RAND_GAIN), maxval=tf.math.log(MAX_RAND_GAIN)))\n",
        "    noisy = random_gain * noisy\n",
        "\n",
        "    noisy_frames = tf.signal.frame(noisy, frame_length, frame_step)\n",
        "    speech_frames = tf.signal.frame(speech, frame_length, frame_step)\n",
        "    noisy_frames = tf.squeeze(noisy_frames)\n",
        "    speech_frames = tf.squeeze(speech_frames)\n",
        "    #noisy_stft = tf.signal.stft(noisy,frame_length,frame_step)\n",
        "    # frame_times = random_start_idx / FS + tf.range(0, NUM_FRAMES) * frame_step / FS + frame_length / FS\n",
        "    frame_times = random_start_idx / FS + tf.range(0, NUM_FRAMES) * frame_step / FS + frame_length / FS\n",
        "    \n",
        "    pitch = tf.squeeze(speech_data[\"pitch\"])    \n",
        "    pitch_confidence = tf.squeeze(speech_data[\"pitch_confidence\"])\n",
        "    #pitch = tf.where(pitch_confidence>config['pitch_confidence_threshold'],pitch,0)\n",
        "    pitch_interpolated = _interpolate_pitch_tf(pitch, frame_times)\n",
        "    pitch_interpolated_cents = _convert_hz_to_cent(pitch_interpolated)\n",
        "    pitch_bins = _calc_y(pitch_interpolated_cents)\n",
        "    return noisy_frames, pitch_bins"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPVV-n86kqJb"
      },
      "source": [
        "a = np.random.randn(184,1)\n",
        "mir_eval.melody.hz2cents(np.squeeze(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0TKt5eSs0SF"
      },
      "source": [
        "## Provide Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiU-wMWPs2dZ"
      },
      "source": [
        "def get_training_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_TR_DIR, file) for file in os.listdir(SPEECH_DATA_TR_DIR)])\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_TR_DIR, file) for file in os.listdir(NOISE_DATA_TR_DIR)])\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "    return dataset_features\n",
        "\n",
        "\n",
        "def get_validation_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_CV_DIR, file) for file in os.listdir(SPEECH_DATA_CV_DIR)])\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_CV_DIR, file) for file in os.listdir(NOISE_DATA_CV_DIR)])\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset_features\n",
        "\n",
        "\n",
        "def get_test_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_TT_DIR, file) for file in os.listdir(SPEECH_DATA_TT_DIR)])\n",
        "    # speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(10).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_TT_DIR, file) for file in os.listdir(NOISE_DATA_TT_DIR)])\n",
        "    # noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(10).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset_features"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdIXYFTDoN1j"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLnYza9woPzS"
      },
      "source": [
        "## DEEP-F0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKx4gLkKjwQ"
      },
      "source": [
        "## DEEP-FO without time component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPzG257AP4uR"
      },
      "source": [
        "MODEL_USED = 'deepf0'\n",
        "LOG_DIR = os.path.join(_DATA_DIR, MODEL_USED, 'logs', TIMESTAMP)\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "CHECKPOINT_DIR = os.path.join(_DATA_DIR, MODEL_USED, 'checkpoints', TIMESTAMP)\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPygi-4sKooq"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling2D, Dropout, Permute, Flatten, Dense, Add, ReLU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resblock(x, filters, kernelsize, dilationrate):\n",
        "    fx = Conv2D(filters, (64, 1), dilation_rate=(dilationrate, 1), padding='same',\n",
        "                   activation='relu', name=\"dilation-conv%d\" % dilationrate)(x)\n",
        "    fx = BatchNormalization()(fx)\n",
        "    fx = Conv2D(filters, 1, padding='same')(fx)\n",
        "    out = Add()([x,fx])\n",
        "    out = ReLU()(out)\n",
        "    return out\n",
        "\n",
        "def get_model_deepf0():\n",
        "    layers = 1\n",
        "    filters = 128\n",
        "    width = 512\n",
        "    strides = (16, 1)\n",
        "    dilation_rate_list = [1, 2, 4, 8]\n",
        "\n",
        "    x = Input(shape=(1024,), name='input', dtype='float32')\n",
        "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\n",
        "\n",
        "\n",
        "    y = Conv2D(filters, (width, 1), strides=strides, padding='same',\n",
        "                activation='relu')(y)\n",
        "    for i in range(len(dilation_rate_list)):\n",
        "        dilation_rate = dilation_rate_list[i]\n",
        "        y = resblock(y, 128, 64, dilation_rate)\n",
        "    y = AveragePooling2D(pool_size=(2, 1), strides=None, padding='valid',\n",
        "                        name=\"conv1d-avgpool\")(y)\n",
        "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
        "    y = Flatten(name=\"flatten\")(y)\n",
        "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
        "   \n",
        "\n",
        "    model = Model(inputs=x, outputs=y)\n",
        "    model.compile('adam', 'binary_crossentropy', metrics=['mse', 'mae'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-SpsYhaCNV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g6YW7mUtA64"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uU1pyCDPm8C7"
      },
      "source": [
        "dataset_training = get_training_data()\n",
        "dataset_validation = get_validation_data()\n",
        "dataset_test = get_test_data()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dzEiCs_tDcJ"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poP21a6TnLHO"
      },
      "source": [
        "model = get_model_deepf0()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "33_wEPK8nNfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21188b0c-a4d5-4167-cba2-a20fdf75317c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 1024)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input-reshape (Reshape)         (None, 1024, 1, 1)   0           input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 64, 1, 128)   65664       input-reshape[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dilation-conv1 (Conv2D)         (None, 64, 1, 128)   1048704     conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 64, 1, 128)   512         dilation-conv1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 64, 1, 128)   16512       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 64, 1, 128)   0           conv2d_36[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_28 (ReLU)                 (None, 64, 1, 128)   0           add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dilation-conv2 (Conv2D)         (None, 64, 1, 128)   1048704     re_lu_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 64, 1, 128)   512         dilation-conv2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 64, 1, 128)   16512       batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 64, 1, 128)   0           re_lu_28[0][0]                   \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_29 (ReLU)                 (None, 64, 1, 128)   0           add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dilation-conv4 (Conv2D)         (None, 64, 1, 128)   1048704     re_lu_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 64, 1, 128)   512         dilation-conv4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 64, 1, 128)   16512       batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 64, 1, 128)   0           re_lu_29[0][0]                   \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_30 (ReLU)                 (None, 64, 1, 128)   0           add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dilation-conv8 (Conv2D)         (None, 64, 1, 128)   1048704     re_lu_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 64, 1, 128)   512         dilation-conv8[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 64, 1, 128)   16512       batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 64, 1, 128)   0           re_lu_30[0][0]                   \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_31 (ReLU)                 (None, 64, 1, 128)   0           add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d-avgpool (AveragePooling2 (None, 32, 1, 128)   0           re_lu_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "transpose (Permute)             (None, 1, 32, 128)   0           conv1d-avgpool[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 4096)         0           transpose[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 360)          1474920     flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,803,496\n",
            "Trainable params: 5,802,472\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9FxwYbUXD3_"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbVgnmcitIjt"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHZ4eMqMnchd",
        "collapsed": true
      },
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/BA_2021/deepf0/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn0JRWrccNJH"
      },
      "source": [
        "# JUST USE IF CONTINUING TRAINING\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/BA_2021/crepe/checkpoints/20210427-145400'\n",
        "LOGDIR = '/content/drive/MyDrive/BA_2021/crepe/logs/20210427-145400'\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/crepe/checkpoints', '20210427-145400', '50-2063.93.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OMUc5cRQnSNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96c7610-706a-4d9b-a90f-830ed0cbec2b"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(LOG_DIR, histogram_freq=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(CHECKPOINT_DIR,'{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
        "\n",
        "callbacks = [checkpoint, tensorboard_callback]\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_training,\n",
        "    steps_per_epoch=750,\n",
        "    epochs=200,\n",
        "    # initial_epoch=30,\n",
        "    verbose = 1,\n",
        "    validation_data = dataset_validation,\n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    callbacks = callbacks)\n",
        "    \n",
        "loss = model.evaluate(dataset_test, steps=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "750/750 [==============================] - 22s 27ms/step - loss: 0.1805 - mse: 0.0190 - mae: 0.0541 - val_loss: 0.2401 - val_mse: 0.0357 - val_mae: 0.0863\n",
            "Epoch 2/200\n",
            "400/750 [===============>..............] - ETA: 7s - loss: 0.1845 - mse: 0.0202 - mae: 0.0573"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg0cbO1z2DTj"
      },
      "source": [
        "inp, outp = next(iter(dataset_test))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgj1DGAaAo0v"
      },
      "source": [
        "pred = model.predict(inp)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxGCoAFhasse"
      },
      "source": [
        "for i in range(len(pred)):\n",
        "    plt.figure(i)\n",
        "    plt.plot(pred[i], 'b')\n",
        "    plt.plot(outp[i], 'g')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVozVO6-vtO3"
      },
      "source": [
        "# Prediction Metrics Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34eg7PQOqI6f",
        "outputId": "d1704e3e-72d3-4c00-c22e-49b057af0842"
      },
      "source": [
        "print(classifier_cents)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0.   20.   40.   60.   80.  100.  120.  140.  160.  180.  200.  220.\n",
            "  240.  260.  280.  300.  320.  340.  360.  380.  400.  420.  440.  460.\n",
            "  480.  500.  520.  540.  560.  580.  600.  620.  640.  660.  680.  700.\n",
            "  720.  740.  760.  780.  800.  820.  840.  860.  880.  900.  920.  940.\n",
            "  960.  980. 1000. 1020. 1040. 1060. 1080. 1100. 1120. 1140. 1160. 1180.\n",
            " 1200. 1220. 1240. 1260. 1280. 1300. 1320. 1340. 1360. 1380. 1400. 1420.\n",
            " 1440. 1460. 1480. 1500. 1520. 1540. 1560. 1580. 1600. 1620. 1640. 1660.\n",
            " 1680. 1700. 1720. 1740. 1760. 1780. 1800. 1820. 1840. 1860. 1880. 1900.\n",
            " 1920. 1940. 1960. 1980. 2000. 2020. 2040. 2060. 2080. 2100. 2120. 2140.\n",
            " 2160. 2180. 2200. 2220. 2240. 2260. 2280. 2300. 2320. 2340. 2360. 2380.\n",
            " 2400. 2420. 2440. 2460. 2480. 2500. 2520. 2540. 2560. 2580. 2600. 2620.\n",
            " 2640. 2660. 2680. 2700. 2720. 2740. 2760. 2780. 2800. 2820. 2840. 2860.\n",
            " 2880. 2900. 2920. 2940. 2960. 2980. 3000. 3020. 3040. 3060. 3080. 3100.\n",
            " 3120. 3140. 3160. 3180. 3200. 3220. 3240. 3260. 3280. 3300. 3320. 3340.\n",
            " 3360. 3380. 3400. 3420. 3440. 3460. 3480. 3500. 3520. 3540. 3560. 3580.\n",
            " 3600. 3620. 3640. 3660. 3680. 3700. 3720. 3740. 3760. 3780. 3800. 3820.\n",
            " 3840. 3860. 3880. 3900. 3920. 3940. 3960. 3980. 4000. 4020. 4040. 4060.\n",
            " 4080. 4100. 4120. 4140. 4160. 4180. 4200. 4220. 4240. 4260. 4280. 4300.\n",
            " 4320. 4340. 4360. 4380. 4400. 4420. 4440. 4460. 4480. 4500. 4520. 4540.\n",
            " 4560. 4580. 4600. 4620. 4640. 4660. 4680. 4700. 4720. 4740. 4760. 4780.\n",
            " 4800. 4820. 4840. 4860. 4880. 4900. 4920. 4940. 4960. 4980. 5000. 5020.\n",
            " 5040. 5060. 5080. 5100. 5120. 5140. 5160. 5180. 5200. 5220. 5240. 5260.\n",
            " 5280. 5300. 5320. 5340. 5360. 5380. 5400. 5420. 5440. 5460. 5480. 5500.\n",
            " 5520. 5540. 5560. 5580. 5600. 5620. 5640. 5660. 5680. 5700. 5720. 5740.\n",
            " 5760. 5780. 5800. 5820. 5840. 5860. 5880. 5900. 5920. 5940. 5960. 5980.\n",
            " 6000. 6020. 6040. 6060. 6080. 6100. 6120. 6140. 6160. 6180. 6200. 6220.\n",
            " 6240. 6260. 6280. 6300. 6320. 6340. 6360. 6380. 6400. 6420. 6440. 6460.\n",
            " 6480. 6500. 6520. 6540. 6560. 6580. 6600. 6620. 6640. 6660. 6680. 6700.\n",
            " 6720. 6740. 6760. 6780. 6800. 6820. 6840. 6860. 6880. 6900. 6920. 6940.\n",
            " 6960. 6980. 7000. 7020. 7040. 7060. 7080. 7100. 7120. 7140. 7160. 7180.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-XJKS-W4Q2D"
      },
      "source": [
        "classifier_lowest_hz = 0.0\n",
        "classifier_lowest_cent = mir_eval.melody.hz2cents(np.array([classifier_lowest_hz]))[0]\n",
        "classifier_cents_per_bin = 20\n",
        "classifier_octaves = 6\n",
        "classifier_total_bins = int((1200 / classifier_cents_per_bin) * classifier_octaves)\n",
        "classifier_cents = np.linspace(0, (classifier_total_bins - 1) * classifier_cents_per_bin, classifier_total_bins) + classifier_lowest_cent\n",
        "\n",
        "def to_weighted_average_cents(label):\n",
        "    if label.ndim == 1:\n",
        "        productsum = np.sum(label * classifier_cents)\n",
        "        weightsum = np.sum(label)\n",
        "        return productsum / weightsum\n",
        "    if label.ndim == 2:\n",
        "        productsum = np.dot(label, classifier_cents)\n",
        "        weightsum = np.sum(label, axis=1)\n",
        "        return productsum / weightsum\n",
        "    raise Exception(\"label should be either 1d or 2d ndarray\")\n",
        "\n",
        "def to_local_average_cents(salience, center=None):\n",
        "    \"\"\"\n",
        "    find the weighted average cents near the argmax bin\n",
        "    \"\"\"\n",
        "    if not hasattr(to_local_average_cents, 'cents_mapping'):\n",
        "        # the bin number-to-cents mapping\n",
        "        to_local_average_cents.cents_mapping = (\n",
        "                np.linspace(0, 7180, 360) + 1997.3794084376191)\n",
        "\n",
        "    if salience.ndim == 1:\n",
        "        if center is None:\n",
        "            center = int(np.argmax(salience))\n",
        "        start = max(0, center - 4)\n",
        "        end = min(len(salience), center + 5)\n",
        "        salience = salience[start:end]\n",
        "        product_sum = np.sum(\n",
        "            salience * to_local_average_cents.cents_mapping[start:end])\n",
        "        weight_sum = np.sum(salience)\n",
        "        return product_sum / weight_sum\n",
        "    if salience.ndim == 2:\n",
        "        return np.array([to_local_average_cents(salience[i, :]) for i in\n",
        "                         range(salience.shape[0])])\n",
        "\n",
        "    raise Exception(\"label should be either 1d or 2d ndarray\")\n",
        "\n",
        "def convert_cent_to_hz(c,fref=10.0):\n",
        "    return fref*2**(c/1200.0)\n",
        "\n",
        "def raw_pitch_accuracy_cent(true_cents, predicted_cents, cent_tolerence=50):\n",
        "    counter_true = 0\n",
        "    counter_false = 0\n",
        "    for i in range(len(true_cents)):\n",
        "        if abs(predicted_cents[i] - true_cents[i]) <= 50.0:\n",
        "            counter_true += 1\n",
        "        else:\n",
        "            counter_false += 1\n",
        "    if counter_true > 0:\n",
        "        result = counter_true / (counter_true + counter_false) * 100\n",
        "    else:\n",
        "        result = 0\n",
        "    return result\n",
        "\n",
        "def raw_pitch_accuracy_hz(true_hz, predicted_hz):\n",
        "    counter_true = 0\n",
        "    counter_false = 0\n",
        "    for i in range(len(true_hz)):\n",
        "        if abs(predicted_hz[i] - true_hz[i]) <= (true_hz[i] * 0.02):\n",
        "            counter_true += 1\n",
        "        else:\n",
        "            counter_false += 1\n",
        "    if counter_true > 0:\n",
        "        result = counter_true / (counter_true + counter_false) * 100\n",
        "    else:\n",
        "        result = 0\n",
        "    return result\n",
        "\n",
        "def standard_deviation_cent(true_cents, predicted_cents):\n",
        "    diff = abs(predicted_cents - true_cents)\n",
        "    avg = np.mean(diff)\n",
        "    diff = np.square(diff - avg)\n",
        "    sum = np.sum(diff)\n",
        "    std_dev = np.sqrt((sum / (len(diff)-1)))\n",
        "    return std_dev\n",
        "\n",
        "def standard_deviation_hz(true_hz, predicted_hz):\n",
        "    diff = abs(predicted_hz - true_hz)\n",
        "    avg = np.mean(diff)\n",
        "    diff = np.square(diff - avg)\n",
        "    sum = np.sum(diff)\n",
        "    std_dev = np.sqrt((sum / (len(diff)-1)))\n",
        "    return std_dev\n",
        "\n",
        "def mean_absolute_error_cent(true_cents, predicted_cents):\n",
        "    diff = abs(predicted_cents - true_cents)\n",
        "    mae = np.mean(diff)\n",
        "    return mae\n",
        "\n",
        "def mean_absolute_error_hz(true_hz, predicted_hz):\n",
        "    diff = abs(predicted_hz - true_hz)\n",
        "    mae = np.mean(diff)\n",
        "    return mae"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O2yL6xTM4iD"
      },
      "source": [
        "def histogram(diff):\n",
        "# def histogram(true_hz, predicted_hz):\n",
        "    #diff = abs(predicted_hz - true_hz)  \n",
        "    n_bins = 250\n",
        "\n",
        "    x = diff\n",
        "    y = true_hz\n",
        "\n",
        "    # fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
        "\n",
        "    # We can set the number of bins with the `bins` kwarg\n",
        "    plt.figure()\n",
        "    plt.hist(x, bins=n_bins)\n",
        "    # axs[0].hist(x, bins=n_bins)\n",
        "    #plt.xlim([-200, 200])\n",
        "    plt.ylim([0, 5000])\n",
        "    plt.xlabel(\"Error in Hertz\")\n",
        "    plt.ylabel(\"Number of Errors\")\n",
        "    plt.show()\n",
        "    # axs[1].hist(x, bins=n_bins)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnoXGQL06WdR"
      },
      "source": [
        "def prediction():\n",
        "    predicted_c = []\n",
        "    true_c = []\n",
        "    for inp, outp in dataset_test:\n",
        "        predicted = model.predict(inp)\n",
        "        true_cents = to_local_average_cents(outp)\n",
        "        true_c.append(true_cents)\n",
        "        predicted_cents = to_local_average_cents(np.squeeze(predicted))\n",
        "        predicted_c.append(predicted_cents)\n",
        "\n",
        "    true_c = np.reshape(np.array(true_c), (1, (len(true_c)*len(true_c[0]))))\n",
        "    true_c = np.squeeze(true_c)\n",
        "    true_hz = convert_cent_to_hz(true_c)\n",
        "    predicted_c = np.reshape(np.array(predicted_c), (1, (len(predicted_c)*len(predicted_c[0]))))\n",
        "    predicted_c = np.squeeze(predicted_c)\n",
        "    predicted_hz = convert_cent_to_hz(predicted_c)\n",
        "    diff = true_hz - predicted_hz\n",
        "    return predicted_hz, true_hz, true_c, predicted_c, diff"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msEYwMhNddcK"
      },
      "source": [
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210428-214611', '50-0.19.hdf5'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh3HIl3f7ONH"
      },
      "source": [
        "predicted_hz, true_hz, true_cent, predicted_cent, diff = prediction()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcrLxYRs5V73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493e70ba-9575-4280-8cc6-ac159f9880ef"
      },
      "source": [
        "print(predicted_hz.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(429824,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUs_jXsA-8h5"
      },
      "source": [
        "# diff = np.abs(diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq6dX6QSpfoY"
      },
      "source": [
        "combined = zip(true_hz, predicted_hz)\n",
        "filtered = [x for x in list(combined) if x[0] > 0]\n",
        "filtered_unzipped = np.array(list(zip(*filtered)))\n",
        "diff_filtered = filtered_unzipped[0] - filtered_unzipped[1]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTXf2ccNfn1W"
      },
      "source": [
        "combined_cent = zip(true_cent, predicted_cent)\n",
        "filtered_cent = [x for x in list(combined_cent) if x[0] > 2072.0]\n",
        "# filtered_c_unzipped = np.array(list(zip(*filtered_cent)))\n",
        "# diff_filtered_cent = filtered_c_unzipped[0] - filtered_c_unzipped[1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH9mUvpffcgW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b8f344db-6b17-4b4c-cb07-b5ff9d935c29"
      },
      "source": [
        "std_dev_hz = np.std(diff_filtered)\n",
        "mae_hz = mean_absolute_error_hz(true_hz=filtered_unzipped[0], predicted_hz=filtered_unzipped[1])\n",
        "mean_hz = np.mean(diff_filtered)\n",
        "median_hz = np.median(diff_filtered)\n",
        "# std_dev, avg = standard_deviation_hz(true_hz=filtered_unzipped[0], predicted_hz=filtered_unzipped[1])\n",
        "rpa_cent = raw_pitch_accuracy_cent(filtered_cent[0], filtered_cent[1])\n",
        "# histo = histogram(diff)\n",
        "quantile_05 = np.quantile(diff_filtered, 0.05)\n",
        "quantile_95 = np.quantile(diff_filtered, 0.95)\n",
        "min = np.min(diff_filtered)\n",
        "max = np.max(diff_filtered)\n",
        "\n",
        "histo_true = histogram([x[0] - x[1] for x in filtered])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbHElEQVR4nO3de9QcdZ3n8feHRBiWCSRg5MQAJjgZGVx3EB65DN4GxhBACcOAC8eViMzE3cU5eHR2CDqCyrAm60FH5qKTHbIGh+EiI5AlzGJEwuzuWS5JCJdwMQ8Ih2SBRBIJwhCX8N0/6teh8tDdVc+Tru7qpz+vc/p01a9u3/715du/ql9VKSIwMzNrZ49eB2BmZvXnZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWqNJkIekpSQ9JWitpVSrbX9IKSevT85RULklXShqW9KCkI3PrmZfmXy9pXpUxm5nZm3WjZfG7EXFERAyl8QXAHRExC7gjjQOcDMxKj/nAdyBLLsClwDHA0cCljQRjZmbd0YvdUHOBpWl4KXB6rvzqyNwNTJY0DTgJWBERWyJiK7ACmNPtoM3MBtnEitcfwI8kBfC3EbEYODAink3TnwMOTMPTgWdyy25IZa3KdyFpPlmLhH322eeoww47rJOvw2zMHtr4Ystp75m+XxcjMWtv9erVP4+Iqc2mVZ0s3h8RGyW9DVgh6bH8xIiIlEh2W0pEiwGGhoZi1apVnVit2W6bsWB5y2mrFp7axUjM2pP0dKtple6GioiN6XkTcBPZMYfn0+4l0vOmNPtG4ODc4gelslblZmbWJZUlC0n7SJrUGAZmAw8Dy4BGj6Z5wC1peBlwbuoVdSzwYtpddTswW9KUdGB7diozM7MuqXI31IHATZIa2/mHiPgfku4DbpB0PvA08PE0/23AKcAw8ApwHkBEbJF0GXBfmu9rEbGlwrjNzGyEypJFRDwJ/HaT8heAE5uUB3BBi3UtAZZ0OkYzMyvHZ3CbmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoUqTxaSJki6X9KtaXympHskDUu6XtKeqXyvND6cps/IrePiVP64pJOqjtnMzHbVjZbFhcCjufFFwLci4jeArcD5qfx8YGsq/1aaD0mHA2cD7wbmAH8jaUIX4jYzs6TSZCHpIOBU4O/SuIATgBvTLEuB09Pw3DROmn5imn8ucF1EbI+InwHDwNFVxm3WKTMWLO91CGYdUXXL4i+APwVeT+MHAL+IiNfS+AZgehqeDjwDkKa/mObfWd5kmZ0kzZe0StKqzZs3d/p1mJkNtMqShaSPApsiYnVV28iLiMURMRQRQ1OnTu3GJs3MBsbECtd9PHCapFOAXwP2Bb4NTJY0MbUeDgI2pvk3AgcDGyRNBPYDXsiVN+SXMTOzLqisZRERF0fEQRExg+wA9U8i4hPAncCZabZ5wC1peFkaJ03/SUREKj879ZaaCcwC7q0qbjMze7MqWxatXARcJ+nPgfuBq1L5VcD3JQ0DW8gSDBGxTtINwCPAa8AFEbGj+2GbmQ2uriSLiFgJrEzDT9KkN1NEvAqc1WL5y4HLq4vQzMza8RncZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVmhwmQh6UJJ+ypzlaQ1kmZ3IzgzM6uHMi2LT0fENmA2MAX4JLCw0qjMzKxWyiQLpedTgO9HxLpcmZnthhkLljNjwfJeh2FWqEyyWC3pR2TJ4nZJk4DXqw3LzMzqZGK7iZIEXAJMBZ6MiFckHQCc143gzMysHtomi4gISbdFxHtyZS8AL1QemZmZ1UaZ3VBrJL2v8kjMzKy22rYskmOAT0h6GniZ7OB2RMS/qTQyMzOrjTLJ4qTKozAzs1or3A0VEU8Dk4GPpcfkVGZmZgOi1BncwDXA29Lj7yX9cdWBmZlZfZTZDXU+cExEvAwgaRHwf4C/rDIwMzOrj7JncO/Ije/AZ3CbmQ2UMi2L/wbcI+mmNH46cFV1IZmZWd0UncG9B3A3sBJ4fyo+LyLurzguMzOrkba7oSLideCvI2JNRFyZHqUShaRfk3SvpAckrZP01VQ+U9I9koYlXS9pz1S+VxofTtNn5NZ1cSp/XJK78pqZdVmZYxZ3SPqDdJ2o0dgOnBARvw0cAcyRdCywCPhWRPwGsJXsADrpeWsq/1aaD0mHA2cD7wbmAH8jacIoYzEzs91QJll8BvgBsF3SNkkvSdpWtFBkfplG35IeAZwA3JjKl5IdAwGYm8ZJ009MCWoucF1EbI+InwHDwNEl4jYzsw5pmyzSMYs5EbFHROwZEftGxKSI2LfMyiVNkLQW2ASsAJ4AfhERr6VZNgDT0/B04BmANP1F4IB8eZNl8tuaL2mVpFWbN28uE56ZmZVU5pjFX4115RGxIyKOAA4iaw0cNtZ1ldjW4ogYioihqVOnVrUZM7OBVOUxi50i4hfAncBxwGRJjV5YBwEb0/BG4GCANH0/skuh7yxvsoyZmXVBZccsJE2VNDkN7w18BHiULGmcmWabB9yShpelcdL0n0REpPKzU2+pmcAs4N5Sr87MzDqi8KS8iJg0xnVPA5amnkt7ADdExK2SHgGuk/TnwP28cYLfVcD3JQ0DW8h6QBER6yTdADwCvAZcEBE7MDOzrmmZLCT9u4j4+zR8fET879y0z0ZE22MZEfEg8N4m5U/SpDdTRLwKnNViXZcDl7fbnpmZVafdbqjP54ZHXjTw0xXEYmZmNdUuWajFcLNxMzMbx9oli2gx3GzczMzGsXYHuA+T9CBZK+KdaZg0fmjlkZmZWW20Sxa/1bUozMys1lomC99n28zMGsqclGdmZgPOycLMzAq1TBaS7kjPi7oXjpmZ1VG7A9zTJP0OcJqk6xhxbkVErKk0MjMzq412yeIS4MtkV3n95ohpjZsYmZnZAGjXG+pG4EZJX46Iy7oYk5mZ1UyZq85eJuk04IOpaGVE3FptWGZmVieFvaEkfR24kOwS4Y8AF0r6z1UHZmZm9VHYsgBOBY5It1hF0lKy+1B8scrAzMysPsqeZzE5N7xfFYGYmVl9lWlZfB24X9KdZN1nPwgsqDQqMzOrlTIHuK+VtBJ4Xyq6KCKeqzQqMzOrlTItCyLiWWBZxbGYmVlN+dpQZmZWyMnCzMwKtU0WkiZIeqxbwZiZWT21TRYRsQN4XNIhXYrHzMxqqMwB7inAOkn3Ai83CiPitMqiMjOzWimTLL5ceRRmZlZrZc6zuEvSO4BZEfFjSf8KmFB9aGZmVhdlLiT4R8CNwN+mounAzVUGZWZm9VKm6+wFwPHANoCIWA+8rcqgzMysXsoki+0R8avGiKSJZHfKMzOzAVEmWdwl6YvA3pI+AvwA+O/VhmU2WGYsWN7rEMzaKpMsFgCbgYeAzwC3AX9WZVBmZlYvZXpDvZ5ueHQP2e6nxyPCu6HMzAZIYbKQdCrwXeAJsvtZzJT0mYj4p6qDMzOzeihzUt4VwO9GxDCApHcCywEnCzOzAVHmmMVLjUSRPAm8VFE8ZmZWQy2ThaQzJJ0BrJJ0m6RPSZpH1hPqvqIVSzpY0p2SHpG0TtKFqXx/SSskrU/PU1K5JF0paVjSg5KOzK1rXpp/fYrBzMy6qN1uqI/lhp8HPpSGNwN7l1j3a8AXImKNpEnAakkrgE8Bd0TEQkkLyHpbXQScDMxKj2OA7wDHSNofuBQYIjvAvlrSsojYWvI1mpnZbmqZLCLivN1ZcboV67Np+CVJj5JdKmQu8OE021JgJVmymAtcnXpa3S1psqRpad4VEbEFICWcOcC1uxOfmZmVV6Y31Ezgj4EZ+flHc4lySTOA95J1vz0wJRKA54AD0/B04JncYhtSWavykduYD8wHOOQQ337DzKyTyvSGuhm4iuxYxeuj3YCkXwf+EfhcRGyTtHNaRISkjpyzERGLgcUAQ0NDPg/EzKyDyiSLVyPiyrGsXNJbyBLFNRHxw1T8vKRpEfFs2s20KZVvBA7OLX5QKtvIG7utGuUrxxKPmZmNTZmus9+WdKmk4yQd2XgULaSsCXEV8GhEfDM3aRnQ6NE0D7glV35u6hV1LPBi2l11OzBb0pTUc2p2KjMzsy4p07J4D/BJ4ATe2A0Vabyd49NyD0lam8q+CCwEbpB0PvA08PE07TbgFGAYeAU4DyAitki6jDe6636tcbDbzMy6o0yyOAs4NH+Z8jIi4n+RXR6kmRObzB9k985otq4lwJLRbN/MzDqnzG6oh4HJVQdiZmb1VaZlMRl4TNJ9wPZG4Wi6zpoNIt+jwsaTMsni0sqjMDOzWitzP4u7uhGImZnVV5kzuF/ijXtu7wm8BXg5IvatMjAzM6uPMi2LSY3hdO7EXODYKoMyM7N6KdMbaqfI3AycVFE8ZmZWQ2V2Q52RG92D7FLhr1YWkZn11IwFy3lq4am9DsNqpkxvqPx9LV4DniLbFWVm41ij668Th0G5Yxa7dV8LMzPrfy2ThaRL2iwXEXFZBfGYmVkNtWtZvNykbB/gfOAAwMnCzGxAtLut6hWN4XQP7QvJrgR7HXBFq+XMzGz8aXvMQtL+wOeBT5DdL/vIiNjajcDMzKw+2h2z+AZwBtmtSt8TEb/sWlRmZlYr7U7K+wLwduDPgP8raVt6vCRpW3fCMzOzOmiZLCJij4jYOyImRcS+ucckXxfKbHD4UusGo7zch5mZDSYnC2tqxoLl/kdpZjs5WZiZWSEnCzN7E7cqbSQnCzMzK+RkYVYTdf43X+fYrDucLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszK81daAeXk4WZmRVysjCzUtyqGGxOFmZmVsjJwszMClWWLCQtkbRJ0sO5sv0lrZC0Pj1PSeWSdKWkYUkPSjoyt8y8NP96SfOqitfMzFqrsmXxPWDOiLIFwB0RMQu4I40DnAzMSo/5wHcgSy7ApcAxwNHApY0EY2Zm3VNZsoiIfwa2jCieCyxNw0uB03PlV0fmbmCypGnAScCKiNgSEVuBFbw5AZlZD/iA92Dp9jGLAyPi2TT8HHBgGp4OPJObb0Mqa1X+JpLmS1oladXmzZs7G7WZ2YCb2KsNR0RIig6ubzGwGGBoaKhj6zWzXblFMZi63bJ4Pu1eIj1vSuUbgYNz8x2UylqVm9kAcGKqj24ni2VAo0fTPOCWXPm5qVfUscCLaXfV7cBsSVPSge3ZqczMzLqost1Qkq4FPgy8VdIGsl5NC4EbJJ0PPA18PM1+G3AKMAy8ApwHEBFbJF0G3Jfm+1pEjDxobmZmFassWUTEOS0mndhk3gAuaLGeJcCSDoZmZmaj5DO4zaxWfJyinpwszGqk1z+Uvd6+1ZeThdmA250E4eQyOHp2noXZeNWPP6C7G3Nj+acWntqJcKyG3LIwM7NCThZWqB//KVv3dKpVYvXmZGFmfcOJpXecLMzMrJCThZXif3Rmg83JwsxqyX9Q6sVdZ620/JfXXSTNBotbFmZmVsjJwt6kH5r/MxYs74s4zcYLJwvrO+M9SfT76ysTv5N9/3GyMBtgVf1gOxGMP04W1tf8ozQ++X2tHyeLATbyC9kPX9C6x1j3+HrN9dO/3HXW2vKX27rFn7V6c8vCzDqmih98HwyvBycLGxf8Y9Kf/L71DycL20U/fnn7MeZB0clWgd/n3nKyMLPKFf3QOxHUnw9wW198UfshRrPxzC0Lq7VBPbg5Hl53N+Pv97rqB04WZh3Sbz9Y/RZvQ7/G3e+cLGy3+ItrNhicLAbQeP2BHw+7brql3+up8V73++voJz7APUD67YvVb/FaOX5f+5NbFuPcIHZZHE+vaTy9lqq4jrrDyWJA7e4XzF/QXbk+irmO+pt3Q9mYVXUdIKvOeK7fGQuW77w3fON1+l7xneOWxTjT7MdgPP9AtDKIr9ne3Mmh3fchf5Dcn5diThZ9rOhL0WreOqriC9ut11z3um3olzg7rdn3pFUSGdQ6KsO7ocahbn/g883/sSxXZbxjja1X6221LfAuld0x1s9Y0XKD9J64ZTGO9MO/ol7cna+qXQ3+Jzo+jeV9HYTPQd+0LCTNAb4NTAD+LiIW9jikyrT7J1nXf5n5L0uruEcefOyFkdtutGxGU5+9ir+ojssua+WMNWHU7bvZKYqIXsdQSNIE4KfAR4ANwH3AORHxSLP5h4aGYtWqVV2MsHOa/SD4i959Ve8e64SRMbZLfHV/Lf2q6Dva6nNU14QiaXVEDDWd1ifJ4jjgKxFxUhq/GCAivt5s/k4ki3b/Ntv9oOfH8x+Udh+qdj9M/fCjZWaj0y6JdPN42EjjIVmcCcyJiD9M458EjomIz+bmmQ/MT6PvAh7vwKbfCvy8A+vptDrG5ZjKq2NcdYwJ6hnXeI7pHRExtdmEvjlmUSQiFgOLO7lOSataZdleqmNcjqm8OsZVx5ignnENakz90htqI3BwbvygVGZmZl3QL8niPmCWpJmS9gTOBpb1OCYzs4HRF7uhIuI1SZ8FbifrOrskItZ1YdMd3a3VQXWMyzGVV8e46hgT1DOugYypLw5wm5lZb/XLbigzM+shJwszMys00MlC0lmS1kl6XdLQiGkXSxqW9Likk3Llc1LZsKQFufKZku5J5denA/G7G9/1ktamx1OS1qbyGZL+JTftu7lljpL0UIrjSkna3ThGxPQVSRtz2z4lN21UddbhuL4h6TFJD0q6SdLkVN6zumoSY+X10GK7B0u6U9Ij6fN+YSof9XtZQWxPpfdgraRVqWx/SSskrU/PU1K50vs0nN7nIyuI5125+lgraZukz/WiriQtkbRJ0sO5slHXjaR5af71kuaNOaCIGNgH8FtkJ/CtBIZy5YcDDwB7ATOBJ8gOrE9Iw4cCe6Z5Dk/L3ACcnYa/C/yHDsd6BXBJGp4BPNxivnuBYwEB/wSc3OE4vgL8SZPyUddZh+OaDUxMw4uARb2uqxHb6ko9tNj2NODINDyJ7NI5h4/2vawotqeAt44o+y/AgjS8IPdenpLeJ6X37Z6K620C8Bzwjl7UFfBB4Mj853e0dQPsDzyZnqek4SljiWegWxYR8WhENDvTey5wXURsj4ifAcPA0ekxHBFPRsSvgOuAuekf6QnAjWn5pcDpnYozrf/jwLUF800D9o2IuyP7pFzdyTgKjKrOOr3xiPhRRLyWRu8mOxenpR7UVVfqoZmIeDYi1qThl4BHgeltFmn1XnbLXLLvEOz6XZoLXB2Zu4HJ6X2syonAExHxdJt5KquriPhnYEuT7Y2mbk4CVkTElojYCqwA5owlnoFOFm1MB57JjW9IZa3KDwB+kfuxapR3ygeA5yNifa5spqT7Jd0l6QO5uDc0ia/TPpuauksazWBGX2dV+jTZv6yGXtZVQy/q4U0kzQDeC9yTikbzXlYhgB9JWq3skj0AB0bEs2n4OeDAHsQF2flc+T9ova4rGH3ddCy+cZ8sJP1Y0sNNHl35V1ekZHznsOuH9lngkIh4L/B54B8k7dulmL4DvBM4IsVxRae2u5txNeb5EvAacE0qqrSu+omkXwf+EfhcRGyjh+9lzvsj4kjgZOACSR/MT0ytvq7371d2zPE04AepqA51tYtu101fnJS3OyLi98awWLvLizQrf4Gs2TcxtS5KX46kKD5JE4EzgKNyy2wHtqfh1ZKeAH4zbTO/+2VMl0UpW2eS/itwaxodbZ2NWom6+hTwUeDE9EWqvK5GoaeXrJH0FrJEcU1E/BAgIp7PTS/7XnZURGxMz5sk3US2C+d5SdMi4tm0K2VTt+MiS15rGnVUh7pKRls3G4EPjyhfOZYNj/uWxRgtA86WtJekmcAssoOhTS87kn6Y7gTOTMvPA27pUCy/BzwWETt3mUiaquweH0g6NMX3ZGqebpN0bDrOcW4H42hsO7+P+PeBRk+NUdVZJ2NKcc0B/hQ4LSJeyZX3rK5G6Nkla9Lruwp4NCK+mSsf7XvZ6bj2kTSpMUzWSeHhtP1Gr538d2kZcG7q+XMs8GJul0yn7dKa73Vd5Yy2bm4HZkuaknadzU5lo9eJo/b9+iB70zeQ/fN8Hrg9N+1LZD0bHifXS4as18FP07Qv5coPJfuQDJM1XffqUIzfA/79iLI/ANYBa4E1wMdy04bIPshPAH9FOku/g3X2feAh4MH0AZ021jrrcFzDZPtm16bHd3tdV01irLweWmz3/WS7Kx7M1c8pY3kvOxzXoWQ9iR5I79GXUvkBwB3AeuDHwP6pXMBfp7geIteDscNx7UO2t2C/3fncdyCOa8l2ef0/st+p88dSN2TH8IbT47yxxuPLfZiZWSHvhjIzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr5GRhA0XSDu16VdHKrv4q6WuSSp8UKunDkm4dUfY9SWe2WqbFek6XdPholjErMu7P4DYb4V8i4oh2M0iaEBE7Wo2XXS4iLtm9UEcvnfF/OtkZxo90e/s2frllYcbO+yoskrQGOKvJ+DnK7rvwsKRFueV+KekKSQ8Ax41Y585WQVrfVyWtSes5bAwxHqXsYoirJd3eOKtY0kpJf6HsfhAXkV3T6Bup5XT8iJbUDknvGHtN2aByy8IGzd5KN5FKvh4R16fhFyK7qB2SFjbGJb2d7LLnRwFbya6SenpE3Ex2tu89EfGFEtv+eVrffwT+BPjDJvN8YER8hwC3pms7/SUwNyI2S/q3wOVkZ+cC7BkRQyn2WcCtEdG4ZP4RqfwC4EPR/pLbZk05Wdigabcb6voW4+8DVkbEZgBJ15DdmOZmYAfZBfrK+GF6Xk12cchm/mdEfLQxIul7afBdwL8GVmSXemIC2aUgWsW+C0nHA39EdukPs1FzsjB7w8sF4828WuZ4RrI9Pe9g9N89Aesi4rgW01vGmnZXXUV2kcVfjnK7ZoCPWZiVcS/wIUlvTVewPQe4q8sxPA5MlXQcZJccl/TuFvO+RHb71MalyX8AXBQRP+1KpDYuOVnYoNl7xAHfhUULRHap5wVkl6F/AFgdEVVezrxZDL8iuwT+onQwfS3wOy1mvw74T5LuT/MMAV/Nvea3dyVoG1d81VkzMyvkloWZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr5GRhZmaF/j+Is8deert65gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhTJzY9A8b7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "2f5503f3-1536-46d5-92ac-6f34ad38af15"
      },
      "source": [
        "std_dev_hz = np.std(diff)\n",
        "mae_hz = mean_absolute_error_hz(true_hz, predicted_hz)\n",
        "mean_hz = np.mean(diff)\n",
        "median_hz = np.median(diff)\n",
        "# std_dev, avg = standard_deviation_hz(true_hz, predicted_hz)\n",
        "rpa_cent = raw_pitch_accuracy_cent(true_cent, predicted_cent)\n",
        "histo = histogram(diff)\n",
        "quantile_05 = np.quantile(diff, 0.05)\n",
        "quantile_95 = np.quantile(diff, 0.95)\n",
        "min = np.min(diff)\n",
        "max = np.max(diff)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCTj31ObAZ4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64236ffb-a3f8-404e-f3bd-4cb18dffe07d"
      },
      "source": [
        "print(\"Stdabweichung:\", \"%.2f\" % std_dev_hz )\n",
        "print(\"Avg in Hz:\", \"%.2f\" % mean_hz)\n",
        "print(\"MAE in Hz:\", \"%.2f\" % mae_hz)\n",
        "print(\"5% Quantil:\", \"%.2f\" % quantile_05)\n",
        "print(\"95% Quantil:\", \"%.2f\" % quantile_95)\n",
        "print(\"Median in Hz:\", \"%.2f\" %median_hz)\n",
        "print(\"Max in Hz:\",\"%.2f\" % max)\n",
        "print(\"Min in Hz:\", \"%.2f\" % min)\n",
        "print(\"RPA in Cent:\", \"%.2f\" % rpa_cent)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stdabweichung: 159.09\n",
            "Avg in Hz: 11.41\n",
            "MAE in Hz: 55.90\n",
            "5% Quantil: -35.81\n",
            "95% Quantil: 317.81\n",
            "Median in Hz: -0.00\n",
            "Max in Hz: 1017.54\n",
            "Min in Hz: -1017.58\n",
            "RPA in Cent: 79.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tfibfpLa5nv"
      },
      "source": [
        "def prediction_metrics():\n",
        "    predicted_c = []\n",
        "    true_c = []\n",
        "    \n",
        "    for inp, outp in dataset_test:\n",
        "        predicted = model.predict(inp)\n",
        "        true_cents = to_local_average_cents(outp)\n",
        "        true_c.append(true_cents)\n",
        "        predicted_cents = to_local_average_cents(np.squeeze(predicted))\n",
        "        predicted_c.append(predicted_cents)\n",
        "    \n",
        "    true_c = np.reshape(np.array(true_c), (1, (len(true_c)*len(true_c[0]))))\n",
        "    true_c = np.squeeze(true_c)\n",
        "    true_hz = convert_cent_to_hz(true_c)\n",
        "    predicted_c = np.reshape(np.array(predicted_c), (1, (len(predicted_c)*len(predicted_c[0]))))\n",
        "    predicted_c = np.squeeze(predicted_c)\n",
        "    predicted_hz = convert_cent_to_hz(predicted_c)\n",
        "    \n",
        "    # Raw Pitch Accuracy\n",
        "    rpa_cent = raw_pitch_accuracy_cent(true_c, predicted_c)\n",
        "    rpa_hz = raw_pitch_accuracy_hz(true_hz, predicted_hz)\n",
        "\n",
        "    # Standard Deviation\n",
        "    std_dev_cent = standard_deviation_cent(true_c, predicted_c)\n",
        "    std_dev_hz = standard_deviation_hz(true_hz, predicted_hz)\n",
        "\n",
        "    # Mean Absolute Error\n",
        "    mae_cent = mean_absolute_error_cent(true_c, predicted_c)\n",
        "    mae_hz = mean_absolute_error_hz(true_hz, predicted_hz)\n",
        "    return rpa_cent, rpa_hz, std_dev_cent, std_dev_hz, mae_cent, mae_hz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOvlZw9tbnl7"
      },
      "source": [
        "pred = prediction_metrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIcO858ubuTv"
      },
      "source": [
        "print(pred[0], pred[1], pred[2], pred[3], pred[4], pred[5])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPA7rTSjMgoc"
      },
      "source": [
        "inp, outp = next(iter(dataset_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZMk4CsCKuc"
      },
      "source": [
        "cents = to_weighted_average_cents(outp)\n",
        "predicted = model.predict(inp, steps=1)\n",
        "predicted = np.squeeze(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}