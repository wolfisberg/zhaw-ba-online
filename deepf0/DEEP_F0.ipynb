{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP-F0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfisberg/zhaw-ba-online/blob/main/deepf0/DEEP_F0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU5ZvlGglGDd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q42BY8BPSoL",
        "collapsed": true
      },
      "source": [
        "!pip install mir_eval\n",
        "!pip install rt_pie\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.interpolate\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import datetime\n",
        "import mir_eval\n",
        "import math\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8nh41ellB7K"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWZIc9-WSItM",
        "collapsed": true
      },
      "source": [
        "# Audio\n",
        "SNR_RANGE = (-5.0,20.0) #dB\n",
        "FRAME_LENGTH = 1024\n",
        "FRAME_STEP = 512\n",
        "MIN_RAND_GAIN = 0.05\n",
        "MAX_RAND_GAIN = 1.1\n",
        "SAMPLE_LENGTH = 3 #shorter than shortest noise/speech sample\n",
        "FS = 16000\n",
        "PITCH_SAMPLING_TIME = 0.01 # s\n",
        "PITCH_FRAME_LENGTH = 0.032 # s\n",
        "\n",
        "\n",
        "# Data\n",
        "BATCH_SIZE = 32\n",
        "NUM_FRAMES = 1 + (FS * SAMPLE_LENGTH - FRAME_LENGTH) // FRAME_STEP\n",
        "# NUM_FRAMES = 1\n",
        "\n",
        "# Training\n",
        "STEPS_PER_EPOCH = 500\n",
        "EPOCHS = 100\n",
        "VALIDATION_STEPS = 5\n",
        "\n",
        "\n",
        "# Directories\n",
        "_DATA_DIR = os.path.join('/content/drive/MyDrive/BA_2021/')\n",
        "_TFRECORDS_DIR = os.path.join(_DATA_DIR, 'tfrecords')\n",
        "\n",
        "SPEECH_DATA_TR_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'tr')\n",
        "NOISE_DATA_TR_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'tr')\n",
        "SPEECH_DATA_CV_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'cv')\n",
        "NOISE_DATA_CV_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'cv')\n",
        "SPEECH_DATA_TT_DIR = os.path.join(_TFRECORDS_DIR, 'speech', 'tt')\n",
        "NOISE_DATA_TT_DIR = os.path.join(_TFRECORDS_DIR, 'noise', 'tt')\n",
        "\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "# Misc\n",
        "SEED = 2\n",
        "\n",
        "\n",
        "# Parsing\n",
        "PARSING_CONFIG_NOISE = {\n",
        "    'data': tf.io.VarLenFeature(tf.string),\n",
        "    'data_sampling_rate': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_num_channels': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_width': tf.io.VarLenFeature(tf.int64),\n",
        "}\n",
        "\n",
        "PARSING_CONFIG_SPEECH = {\n",
        "    'data': tf.io.VarLenFeature(tf.string),\n",
        "    'data_sampling_rate': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_num_channels': tf.io.VarLenFeature(tf.int64),\n",
        "    'data_width': tf.io.VarLenFeature(tf.int64),\n",
        "    'pitch': tf.io.VarLenFeature(tf.float32),\n",
        "    'pitch_confidence': tf.io.VarLenFeature(tf.float32),\n",
        "}\n",
        "\n"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-kwHYrpmCgl"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmb9w4ACwoUi"
      },
      "source": [
        "## Copy Data to Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYKlRsmDwsKM"
      },
      "source": [
        "DATA_DIR_LOCAL = '/content/data'\n",
        "\n",
        "if not os.path.exists(DATA_DIR_LOCAL):\n",
        "    os.mkdir(DATA_DIR_LOCAL)\n",
        "    \n",
        "    RECORD_DIR_LOCAL = os.path.join(DATA_DIR_LOCAL, 'tfrecords')\n",
        "    shutil.copytree(_TFRECORDS_DIR, RECORD_DIR_LOCAL)\n",
        "\n",
        "\n",
        "_TFRECORDS_DIR = os.path.join(DATA_DIR_LOCAL, 'tfrecords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlkFt3Nvsqn-"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhwJchGumFSo"
      },
      "source": [
        "def _parse_noise_record(serialized_example):\n",
        "    parsed_features = tf.io.parse_single_example(serialized_example, features=PARSING_CONFIG_NOISE)\n",
        "    decoded_features = {\n",
        "        \"data_num_channels\": tf.cast(parsed_features[\"data_num_channels\"].values[0], tf.int32),\n",
        "        \"data_sampling_rate\": tf.cast(parsed_features[\"data_sampling_rate\"].values[0], tf.int32),\n",
        "        \"data_width\": tf.cast(parsed_features[\"data_width\"].values[0], tf.int32),\n",
        "    }\n",
        "    data = tf.io.decode_raw(parsed_features['data'].values[0], tf.int16)\n",
        "    decoded_features.update({\"data\": data})\n",
        "    return decoded_features\n",
        "\n",
        "\n",
        "def _parse_speech_record(serialized_example):\n",
        "    parsed_features = tf.io.parse_single_example(serialized_example, features=PARSING_CONFIG_SPEECH)\n",
        "    decoded_features = {\n",
        "        \"data_num_channels\": tf.cast(parsed_features[\"data_num_channels\"].values[0], tf.int32),\n",
        "        \"data_sampling_rate\": tf.cast(parsed_features[\"data_sampling_rate\"].values[0], tf.int32),\n",
        "        \"data_width\": tf.cast(parsed_features[\"data_width\"].values[0], tf.int32),\n",
        "        \"pitch\": tf.cast(parsed_features['pitch'].values, tf.float32),\n",
        "        \"pitch_confidence\": tf.cast(parsed_features['pitch_confidence'].values, tf.float32),\n",
        "    }\n",
        "    data = tf.io.decode_raw(parsed_features['data'].values[0], tf.int16)\n",
        "    decoded_features.update({\"data\": data})\n",
        "    return decoded_features\n",
        "\n",
        "\n",
        "def _mix_noisy_speech(speech, noise):\n",
        "    speech_pow = tf.math.reduce_euclidean_norm(speech)\n",
        "    noise_pow = tf.math.reduce_euclidean_norm(noise)\n",
        "\n",
        "    min_SNR = SNR_RANGE[0]\n",
        "    max_SNR = SNR_RANGE[1]\n",
        "    snr_current = 20.0*tf.math.log(speech_pow/noise_pow)/tf.math.log(10.0)\n",
        "    snr_target = tf.random.uniform((),minval=min_SNR,maxval=max_SNR)\n",
        "\n",
        "    noise = noise * tf.math.pow(10.0,(snr_current-snr_target)/20.0)\n",
        "    noisy_speech = speech+noise\n",
        "\n",
        "    return speech, noise, noisy_speech\n",
        "\n",
        "\n",
        "def _interpolate_pitch(pitch,t):\n",
        "    pitches = pitch.numpy()\n",
        "    t = t.numpy()\n",
        "    t_pitch = np.arange(0, len(pitch)) * PITCH_SAMPLING_TIME + PITCH_FRAME_LENGTH / 2\n",
        "    f = scipy.interpolate.interp1d(t_pitch, pitch, 'nearest')\n",
        "    return f(t).astype(np.float32)\n",
        "\n",
        "def convert_hz_to_cent(f,fref=10.0):\n",
        "    return mir_eval.melody.hz2cents(np.array(f), fref)\n",
        "\n",
        "def calc_bin(freq_cent, cents_per_bin = 20, lower_bound_freq=32.7):  \n",
        "    freq_cent = np.squeeze(freq_cent)\n",
        "    lower_bound_freq_cent = mir_eval.melody.hz2cents(np.array([lower_bound_freq]))\n",
        "    bin = (freq_cent - lower_bound_freq_cent) / np.array([cents_per_bin])\n",
        "    return np.clip(bin, 0, 359)\n",
        "\n",
        "def calc_y(f_groundtruth, n_bins = 360):\n",
        "    c_true = calc_bin(f_groundtruth)\n",
        "    return create_bin_vector(c_true)\n",
        "\n",
        "def create_bin_vector(c_true):\n",
        "    cis = np.arange(360)\n",
        "    y = [gaussian_blur(cis, i) for i in c_true]\n",
        "    return np.squeeze(y)\n",
        "    \n",
        "def gaussian_blur(ci, ctrue):\n",
        "    return np.exp(-(ci-ctrue)**2/(2.0*25.0**2))\n",
        "\n",
        "@tf.function\n",
        "def _interpolate_pitch_tf(pitch,t):\n",
        "    y = tf.py_function(_interpolate_pitch,[pitch,t], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "@tf.function\n",
        "def _convert_hz_to_cent(pitch):\n",
        "    y = tf.py_function(convert_hz_to_cent,[pitch], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "@tf.function\n",
        "def _calc_y(pitch_cents):\n",
        "    y = tf.py_function(calc_y,[pitch_cents], Tout=tf.float32)\n",
        "    return tf.squeeze(y)\n",
        "\n",
        "def _calc_features(speech_data, noise_data):\n",
        "    speech = tf.squeeze(tf.cast(speech_data[\"data\"], tf.float32))\n",
        "    noise = tf.squeeze(tf.cast(noise_data[\"data\"], tf.float32))\n",
        "    speech = speech / tf.int16.max\n",
        "    noise = noise / tf.int16.max\n",
        "\n",
        "    random_start_idx = int(tf.round(tf.random.uniform([], maxval=(\n",
        "             tf.cast(len(noise), tf.float32) - SAMPLE_LENGTH * FS - PITCH_SAMPLING_TIME))))\n",
        "    noise = noise[random_start_idx:random_start_idx + SAMPLE_LENGTH * FS]\n",
        "\n",
        "    random_start_idx = int(tf.round(tf.random.uniform([], minval=161, maxval=(\n",
        "            tf.cast(len(speech), tf.float32) - SAMPLE_LENGTH * FS - 161))))\n",
        "    speech = speech[random_start_idx:random_start_idx + SAMPLE_LENGTH * FS]   \n",
        "\n",
        "    #SNR_range = SNR_RANGE\n",
        "    frame_length = FRAME_LENGTH\n",
        "    frame_step = FRAME_STEP\n",
        "    speech, noise, noisy = _mix_noisy_speech(speech, noise)\n",
        "\n",
        "    random_gain = tf.math.exp(\n",
        "        tf.random.uniform([], minval=tf.math.log(MIN_RAND_GAIN), maxval=tf.math.log(MAX_RAND_GAIN)))\n",
        "    noisy = random_gain * noisy\n",
        "\n",
        "    noisy_frames = tf.signal.frame(noisy, frame_length, frame_step)\n",
        "    speech_frames = tf.signal.frame(speech, frame_length, frame_step)\n",
        "    noisy_frames = tf.squeeze(noisy_frames)\n",
        "    speech_frames = tf.squeeze(speech_frames)\n",
        "    #noisy_stft = tf.signal.stft(noisy,frame_length,frame_step)\n",
        "    frame_times = random_start_idx / FS + tf.range(0, NUM_FRAMES) * frame_step / FS + frame_length / FS\n",
        "    \n",
        "    pitch = tf.squeeze(speech_data[\"pitch\"])    \n",
        "    pitch_confidence = tf.squeeze(speech_data[\"pitch_confidence\"])\n",
        "    #pitch = tf.where(pitch_confidence>config['pitch_confidence_threshold'],pitch,0)\n",
        "    pitch_interpolated = _interpolate_pitch_tf(pitch, frame_times)\n",
        "    pitch_interpolated_cents = _convert_hz_to_cent(pitch_interpolated)\n",
        "    pitch_bins = _calc_y(pitch_interpolated_cents)\n",
        "    return noisy_frames, pitch_bins"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0TKt5eSs0SF"
      },
      "source": [
        "## Provide Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiU-wMWPs2dZ"
      },
      "source": [
        "def get_training_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_TR_DIR, file) for file in os.listdir(SPEECH_DATA_TR_DIR)])\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_TR_DIR, file) for file in os.listdir(NOISE_DATA_TR_DIR)])\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "    return dataset_features\n",
        "\n",
        "\n",
        "def get_validation_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_CV_DIR, file) for file in os.listdir(SPEECH_DATA_CV_DIR)])\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_CV_DIR, file) for file in os.listdir(NOISE_DATA_CV_DIR)])\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset_features\n",
        "\n",
        "\n",
        "def get_test_data():\n",
        "    speech_ds = tf.data.TFRecordDataset([os.path.join(SPEECH_DATA_TT_DIR, file) for file in os.listdir(SPEECH_DATA_TT_DIR)])\n",
        "    # speech_ds = speech_ds.map(_parse_speech_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "    speech_ds = speech_ds.map(_parse_speech_record).repeat(10).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "\n",
        "    noise_ds = tf.data.TFRecordDataset([os.path.join(NOISE_DATA_TT_DIR, file) for file in os.listdir(NOISE_DATA_TT_DIR)])\n",
        "    # noise_ds = noise_ds.map(_parse_noise_record).repeat(None).shuffle(buffer_size=1000, seed=SEED)\n",
        "    noise_ds = noise_ds.map(_parse_noise_record).repeat(10).shuffle(buffer_size=1000, seed=SEED)\n",
        "\n",
        "\n",
        "    dataset_combined = tf.data.Dataset.zip((speech_ds, noise_ds))\n",
        "    dataset_features = dataset_combined.map(_calc_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset_features = dataset_features.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # just use if crepe without time component\n",
        "    dataset_features = dataset_features.unbatch().unbatch().shuffle(3000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset_features"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdIXYFTDoN1j"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKx4gLkKjwQ"
      },
      "source": [
        "## DEEP-F0 without time component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPygi-4sKooq"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling2D, Dropout, Permute, Flatten, Dense, Add, ReLU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def resblock(x, filters, kernelsize, dilationrate, name):\n",
        "    fx = Conv2D(filters, (64, 1), dilation_rate=(dilationrate, 1), padding='same',\n",
        "                   activation='relu', name=\"dilation-conv%d\" % name)(x)\n",
        "    fx = BatchNormalization()(fx)\n",
        "    fx = Conv2D(filters, 1, padding='same')(fx)\n",
        "    out = Add()([x,fx])\n",
        "    out = ReLU()(out)\n",
        "    return out\n",
        "\n",
        "def get_model_deepf0():\n",
        "    layers = 1\n",
        "    filters = 128\n",
        "    width = 512\n",
        "    strides = (16, 1)\n",
        "    dilation_rate = 8\n",
        "\n",
        "    x = Input(shape=(FRAME_LENGTH,), name='input', dtype='float32')\n",
        "    y = Reshape(target_shape=(FRAME_LENGTH, 1, 1), name='input-reshape')(x)\n",
        "\n",
        "\n",
        "    y = Conv2D(filters, (width, 1), strides=strides, padding='same',\n",
        "                activation='relu')(y)\n",
        "    for i in range(4):\n",
        "        y = resblock(y, 128, 64, dilation_rate, name=i)\n",
        "    y = AveragePooling2D(pool_size=(2, 1), strides=None, padding='valid',\n",
        "                        name=\"conv1d-avgpool\")(y)\n",
        "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
        "    y = Flatten(name=\"flatten\")(y)\n",
        "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
        "   \n",
        "\n",
        "    model = Model(inputs=x, outputs=y)\n",
        "    model.compile('adam', 'binary_crossentropy', metrics=['mse', 'mae'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-SpsYhaCNV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g6YW7mUtA64"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uU1pyCDPm8C7"
      },
      "source": [
        "dataset_training = get_training_data()\n",
        "dataset_validation = get_validation_data()\n",
        "dataset_test = get_test_data()"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dzEiCs_tDcJ"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poP21a6TnLHO"
      },
      "source": [
        "model = get_model_deepf0()"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33_wEPK8nNfC"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9FxwYbUXD3_"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbVgnmcitIjt"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHZ4eMqMnchd",
        "collapsed": true
      },
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/BA_2021/deepf0/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPzG257AP4uR"
      },
      "source": [
        "# USE IF IT IS INITIAL TRAINING\n",
        "\n",
        "MODEL_USED = 'deepf0'\n",
        "LOG_DIR = os.path.join(_DATA_DIR, MODEL_USED, 'logs', TIMESTAMP + '_256_128_100_Epochs')\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "CHECKPOINT_DIR = os.path.join(_DATA_DIR, MODEL_USED, 'checkpoints', TIMESTAMP + '_256_128_100_Epochs')\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn0JRWrccNJH"
      },
      "source": [
        "# JUST USE IF CONTINUING TRAINING\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/BA_2021/crepe/checkpoints/20210427-145400'\n",
        "LOGDIR = '/content/drive/MyDrive/BA_2021/crepe/logs/20210427-145400'\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/crepe/checkpoints', '20210427-145400', '50-2063.93.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OMUc5cRQnSNQ"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(LOG_DIR, histogram_freq=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(CHECKPOINT_DIR,'{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
        "early_stopping =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=32, mode='min')\n",
        "\n",
        "callbacks = [checkpoint, tensorboard_callback, early_stopping]\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_training,\n",
        "    steps_per_epoch=500,\n",
        "    epochs=100,\n",
        "    # initial_epoch=30,\n",
        "    verbose = 1,\n",
        "    validation_data = dataset_validation,\n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    callbacks = callbacks)\n",
        "    \n",
        "loss = model.evaluate(dataset_test, steps=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVozVO6-vtO3"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msEYwMhNddcK"
      },
      "source": [
        "# 1024 / 512\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210502-204318_1024_512_100_Epochs', '87-0.16.hdf5'))"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb4YnOFA_dqZ"
      },
      "source": [
        "# 512 / 256\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210503-142217_512_256_100_Epochs', '48-0.22.hdf5'))"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM3NbC0mkbmg"
      },
      "source": [
        "# 256 / 128\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/BA_2021/deepf0/checkpoints', '20210503-193625_256_128_100_Epochs', '91-0.22.hdf5'))"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ucPJnobUMw"
      },
      "source": [
        "from rt_pie.utils import converters\n",
        "def prediction():\n",
        "    predicted_c = []\n",
        "    true_c = []\n",
        "    inp_vector = []\n",
        "    predicted_vector = []\n",
        "    for inp, outp in dataset_test:\n",
        "        predicted = model.predict(inp)\n",
        "        predicted_vector.append(predicted)\n",
        "        inp_vector.append(outp) \n",
        "        true_cents = converters.convert_bin_to_local_average_cents(outp)\n",
        "        true_c.append(true_cents)\n",
        "        predicted_cents = converters.convert_bin_to_local_average_cents(np.squeeze(predicted))\n",
        "        predicted_c.append(predicted_cents)\n",
        "\n",
        "    predicted_vector = np.reshape(np.array(predicted_vector), ((len(predicted_vector) * len(predicted_vector[0]), 360)))\n",
        "    inp_vector = np.reshape(np.array(inp_vector), ((len(inp_vector) * len(inp_vector[0]), 360)))\n",
        "    \n",
        "    true_c = np.reshape(np.array(true_c), (1, (len(true_c)*len(true_c[0]))))\n",
        "    true_c = np.squeeze(true_c)\n",
        "    true_hz = converters.convert_cent_to_hz(true_c)\n",
        "    predicted_c = np.reshape(np.array(predicted_c), (1, (len(predicted_c)*len(predicted_c[0]))))\n",
        "    predicted_c = np.squeeze(predicted_c)\n",
        "    predicted_hz = converters.convert_cent_to_hz(predicted_c)\n",
        "    diff = true_hz - predicted_hz\n",
        "    return predicted_hz, true_hz, true_c, predicted_c, diff, inp_vector, predicted_vector"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh3HIl3f7ONH"
      },
      "source": [
        "predicted_hz, true_hz, true_cent, predicted_cent, diff, inp_vector, predicted_vector = prediction()"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4LPGpFaYwvU"
      },
      "source": [
        "### Prediction Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUs_jXsA-8h5"
      },
      "source": [
        "## Hertz filter\n",
        "combined = zip(true_hz, predicted_hz)\n",
        "filtered = [x for x in list(combined) if x[0] > 60 and x[1] > 60 and x[1] < 300]\n",
        "filtered_unzipped = np.array(list(zip(*filtered)))\n",
        "diff_filtered = filtered_unzipped[0] - filtered_unzipped[1]\n",
        "\n",
        "## Cent filter\n",
        "combined_cent = zip(true_cent, predicted_cent)\n",
        "filtered_cent = [x for x in list(combined_cent) if x[0] > 3101.95500087 and x[1] > 3101.95500087]\n",
        "filtered_c_unzipped = np.array(list(zip(*filtered_cent)))\n",
        "diff_filtered_cent = filtered_c_unzipped[0] - filtered_c_unzipped[1]"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAMZz2UdkylA"
      },
      "source": [
        "## Hz Values with factor 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNSM6rADKWzQ"
      },
      "source": [
        "counter = 0\n",
        "trues = np.array([])\n",
        "preds = np.array([])\n",
        "for i in range(len(diff_filtered)):\n",
        "    if diff_filtered[i] < -50:\n",
        "        # plt.figure(i)\n",
        "        # plt.plot(outp[i], 'r')\n",
        "        # plt.plot(predicted[i], 'b')\n",
        "        # plt.plot(np.argmax(outp[i]),np.max(outp[i]),'x')\n",
        "        # plt.plot(np.argmax(predicted[i]),np.max(predicted[i]),'x')\n",
        "        # plt.text(np.argmax(outp[i])+10,np.max(outp[i]),f'max={np.max(outp[i]):.1f} @ bin {np.argmax(outp[i])}')\n",
        "        # plt.text(np.argmax(predicted[i])+10,np.max(predicted[i]),f'max={np.max(predicted[i]):.1f} @ bin {np.argmax(predicted[i])}')\n",
        "        # plt.show()\n",
        "        trues = np.append(trues, filtered_unzipped[0][i])\n",
        "        preds = np.append(preds, filtered_unzipped[1][i])\n",
        "        # counter += 1\n",
        "        # if counter == 60:\n",
        "        #     break   "
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRJb_B_ryvm5",
        "outputId": "08b9c6da-043a-47e4-8d8a-5ee05f369f17"
      },
      "source": [
        "divided = np.divide(preds, trues)\n",
        "np.mean(divided)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.621543653541332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-AXsfqli1Nl"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr6y9h0_cCRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "7dfa4ea0-a669-4378-814c-5f2399633100"
      },
      "source": [
        "from rt_pie.utils import metrics\n",
        "hz_metrics = metrics.get_hz_metrics(filtered_unzipped[0], filtered_unzipped[1], print_output=True, rpa_relative_tolerance=0.05)\n",
        "rpa_cent = metrics.raw_pitch_accuracy_cent(filtered_c_unzipped[0], filtered_c_unzipped[1])\n",
        "print(rpa_cent)\n",
        "hist = histogram(diff_filtered)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min abs err [Hz] _______ 0.0\n",
            "Max abs err [Hz] _____ 265.7\n",
            "Mean err [Hz] _______ -13.84\n",
            "Median [Hz] __________ -0.01\n",
            "MAE [Hz] _____________ 18.91\n",
            "StdDev [Hz] __________ 44.36\n",
            "5% quant err [Hz] __ -138.79\n",
            "95% quant err [Hz] ___ 10.31\n",
            "RPA [Hz] _____________ 78.24\n",
            "67.80213757193752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y3oZ8LQKZV9"
      },
      "source": [
        "hz_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ2347buZISB"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qtgyIEBiJz7"
      },
      "source": [
        "def histogram(diff):  \n",
        "    n_bins = 250\n",
        "    x = diff\n",
        "    y = true_hz\n",
        "\n",
        "    plt.figure()\n",
        "    plt.hist(x, bins=n_bins)\n",
        "    #plt.xlim([-200, 200])\n",
        "    plt.ylim([0, 10000])\n",
        "    plt.axvline(np.median(x), color='k', linestyle='dashed', linewidth=2, label='median')\n",
        "    plt.axvline(np.mean(x), color='k', linestyle='solid', linewidth=2, label='mean')\n",
        "    plt.axvline(np.quantile(x, 0.05), color='k', linestyle='dotted', linewidth=2, label='5% quantile')\n",
        "    plt.axvline(np.quantile(x, 0.95), color='k', linestyle='dashdot', linewidth=2, label='95% quantile')\n",
        "    plt.xlabel(\"Error in Hertz\")\n",
        "    plt.ylabel(\"Number of Errors\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# histo = histogram(diff)\n",
        "# histo_true = histogram([x[0] - x[1] for x in filtered])"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFxKN2zZa_-s"
      },
      "source": [
        "# Debug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM1IhBo8sNwT"
      },
      "source": [
        "err = np.array([x[0]-x[1] for x in filtered])\n",
        "gt = np.array([x[0] for x in filtered])\n",
        "est = np.array([x[1] for x in filtered])\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(gt,err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60kyTZSnvfOk"
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(gt[err < -50],est[err < -50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_ozKh80tFub"
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(gt[est<35],err[est<35])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgj1DGAaAo0v"
      },
      "source": [
        "pred = model.predict(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxGCoAFhasse"
      },
      "source": [
        "for i in range(len(pred)):\n",
        "    plt.figure(i)\n",
        "    plt.plot(pred[i], 'b')\n",
        "    plt.plot(outp[i], 'g')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8J5dEclpXSG"
      },
      "source": [
        "pred = model.predict(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQrpOxrJpbhm"
      },
      "source": [
        "for i in range(len(pred)):\n",
        "    plt.figure(i)\n",
        "    plt.plot(pred[i], 'g')\n",
        "    plt.plot(out[i], 'b')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxNitqcPiLd3"
      },
      "source": [
        "for i in range(len(pred)):\n",
        "    plt.figure(i)\n",
        "    z = pred[i]\n",
        "    y = out[i]\n",
        "    plt.figure()\n",
        "    plt.plot(z, 'b')\n",
        "    plt.plot(y, 'r')\n",
        "    plt.plot(np.argmax(z),np.max(z),'x')\n",
        "    plt.plot(np.argmax(y),np.max(y),'x')\n",
        "    plt.ylim([0, 1.1])\n",
        "    plt.text(np.argmax(z)+10,np.max(z),f'max={np.max(z):.1f} @ bin {np.argmax(z)}')\n",
        "    plt.text(np.argmax(y)+10,np.max(y),f'max={np.max(y):.1f} @ bin {np.argmax(y)}')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}